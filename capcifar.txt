[Time] - SPS.PSM: 0.0019 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6880696614583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97216796875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97216796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9718424479166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97216796875
sparsity Attention k_chunks: 0.97216796875
sparsity Attention v_chunks: 0.9718424479166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.97216796875
sparsity Attention postReshape k_chunks: 0.97216796875
sparsity Attention postReshape v_chunks: 0.9718424479166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9805501302083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9944661458333334
sparsity Attention postReshape chunk - 1 output: 0.9918619791666666
sparsity Attention postReshape chunk - 2 output: 0.9602864583333334
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6902669270833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9705607096354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6845703125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97119140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9694010416666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96923828125
sparsity Attention k_chunks: 0.97119140625
sparsity Attention v_chunks: 0.9694010416666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96923828125
sparsity Attention postReshape k_chunks: 0.97119140625
sparsity Attention postReshape v_chunks: 0.9694010416666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9773763020833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9938151041666666
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9615885416666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.67919921875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9708455403645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6768391927083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9700520833333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.9700520833333334
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.9700520833333334
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.968505859375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9931640625
sparsity Attention postReshape chunk - 1 output: 0.9791666666666666
sparsity Attention postReshape chunk - 2 output: 0.955078125
sparsity Attention postReshape chunk - 3 output: 0.9466145833333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6756998697916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695231119791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.6705729166666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9681803385416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9707845052083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9671223958333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9681803385416666
sparsity Attention k_chunks: 0.9707845052083334
sparsity Attention v_chunks: 0.9671223958333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9681803385416666
sparsity Attention postReshape k_chunks: 0.9707845052083334
sparsity Attention postReshape v_chunks: 0.9671223958333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9751790364583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.96484375
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6625162760416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674479166666666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6609700520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.97802734375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.974609375
sparsity Attention postReshape chunk - 2 output: 0.9811197916666666
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9947916666666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6604817708333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677937825520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.65380859375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9669596354166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.9669596354166666
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.9669596354166666
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.974609375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.9664713541666666
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6548665364583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690144856770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.658203125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9679361979166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676106770833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966796875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9679361979166666
sparsity Attention k_chunks: 0.9676106770833334
sparsity Attention v_chunks: 0.966796875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9679361979166666
sparsity Attention postReshape k_chunks: 0.9676106770833334
sparsity Attention postReshape v_chunks: 0.966796875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9754231770833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9674479166666666
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0034 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.654052734375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9666341145833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.6498209635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9667154947916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9667154947916666
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9667154947916666
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.974853515625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9720052083333334
sparsity Attention postReshape chunk - 1 output: 0.9752604166666666
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0034 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6499837239583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.967041015625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
[Time] - Spikeformer Time time: 0.0442 seconds
Run 1 - Tempo totale del modello: 0.0442 secondi
[Time] - SPS.PSM: 0.0014 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6798502604166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9706217447916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9710286458333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.9706217447916666
sparsity Attention v_chunks: 0.9710286458333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.9706217447916666
sparsity Attention postReshape v_chunks: 0.9710286458333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9912109375
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.9625651041666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6883951822916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9697672526041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6791178385416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97021484375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97021484375
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.97021484375
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9781901041666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9921875
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0034 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6775716145833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9691365559895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.6665852864583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970947265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970947265625
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.970947265625
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9788411458333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9918619791666666
sparsity Attention postReshape chunk - 1 output: 0.9860026041666666
sparsity Attention postReshape chunk - 2 output: 0.9609375
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6666666666666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686279296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.6609700520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968994140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968994140625
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968994140625
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9815266927083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9912109375
sparsity Attention postReshape chunk - 1 output: 0.9886067708333334
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9954427083333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0034 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.662109375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680989583333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.65625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9659016927083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.9659016927083334
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.9659016927083334
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9771321614583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9664713541666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6561686197916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673868815104166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.6519368489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968017578125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9658203125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.968017578125
sparsity Attention v_chunks: 0.9658203125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.968017578125
sparsity Attention postReshape v_chunks: 0.9658203125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.971923828125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9742838541666666
sparsity Attention postReshape chunk - 1 output: 0.9723307291666666
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0034 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6514485677083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671630859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.6473795572916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9705403645833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9666341145833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.9705403645833334
sparsity Attention v_chunks: 0.9666341145833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.9705403645833334
sparsity Attention postReshape v_chunks: 0.9666341145833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9756673177083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9759114583333334
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9781901041666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0034 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6474609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96636962890625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.6484375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96630859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.970703125
sparsity Attention v_chunks: 0.96630859375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.970703125
sparsity Attention postReshape v_chunks: 0.96630859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9755045572916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.97265625
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9964192708333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0034 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.646484375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.966552734375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
[Time] - Spikeformer Time time: 0.0423 seconds
Run 2 - Tempo totale del modello: 0.0423 secondi
[Time] - SPS.PSM: 0.0019 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6975911458333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970947265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9703776041666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9694010416666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970947265625
sparsity Attention k_chunks: 0.9703776041666666
sparsity Attention v_chunks: 0.9694010416666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.970947265625
sparsity Attention postReshape k_chunks: 0.9703776041666666
sparsity Attention postReshape v_chunks: 0.9694010416666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9795735677083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9915364583333334
sparsity Attention postReshape chunk - 1 output: 0.9879557291666666
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9625651041666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9934895833333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6822916666666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9702962239583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6725260416666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970947265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.970947265625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.970947265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9766438802083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9892578125
sparsity Attention postReshape chunk - 1 output: 0.9908854166666666
sparsity Attention postReshape chunk - 2 output: 0.9583333333333334
sparsity Attention postReshape chunk - 3 output: 0.9680989583333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6707356770833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679158528645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6665852864583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96728515625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.96728515625
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.96728515625
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9706217447916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9638671875
sparsity Attention postReshape chunk - 3 output: 0.9514973958333334
[Time] - Transpose and LIF: 0.0012 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0045 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6629231770833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683430989583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6571451822916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.977783203125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.9895833333333334
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6544596354166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679972330729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6499837239583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9681803385416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9710286458333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9681803385416666
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.9710286458333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9681803385416666
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.9710286458333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9793294270833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9811197916666666
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9807942708333334
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6503092447916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675496419270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6499837239583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9663899739583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.9663899739583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.9663899739583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9750162760416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.9736328125
sparsity Attention postReshape chunk - 3 output: 0.958984375
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6480305989583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670613606770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6490071614583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966552734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9654947916666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966552734375
sparsity Attention k_chunks: 0.9654947916666666
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966552734375
sparsity Attention postReshape k_chunks: 0.9654947916666666
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.976318359375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9817708333333334
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9677734375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.646484375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665120442708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6441243489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9765625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9775390625
sparsity Attention postReshape chunk - 1 output: 0.984375
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6444498697916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681599934895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
[Time] - Spikeformer Time time: 0.0477 seconds
Run 3 - Tempo totale del modello: 0.0477 secondi
[Time] - SPS.PSM: 0.0016 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6874186197916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9705403645833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9680989583333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9705403645833334
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.9680989583333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9705403645833334
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.9680989583333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9746907552083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.990234375
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.958984375
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9934895833333334
sparsity Attention postReshape chunk - 3 x: 0.9954427083333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6874186197916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96905517578125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6778971354166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9679361979166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.9679361979166666
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.9679361979166666
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9745279947916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9654947916666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6748860677083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.967529296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6669108072916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9672037760416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.9672037760416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.9672037760416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9744466145833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9593098958333334
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6619466145833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671223958333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6569010416666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9777018229166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6549479166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9655558268229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6549479166666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967529296875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9664713541666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967529296875
sparsity Attention k_chunks: 0.9664713541666666
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.967529296875
sparsity Attention postReshape k_chunks: 0.9664713541666666
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9745279947916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9749348958333334
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9641927083333334
sparsity Attention postReshape chunk - 3 output: 0.9820963541666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6474609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9668986002604166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6439615885416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9654947916666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9669596354166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9672037760416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9654947916666666
sparsity Attention k_chunks: 0.9669596354166666
sparsity Attention v_chunks: 0.9672037760416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9654947916666666
sparsity Attention postReshape k_chunks: 0.9669596354166666
sparsity Attention postReshape v_chunks: 0.9672037760416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9746907552083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6434733072916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96636962890625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6420084635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9644368489583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9659016927083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9644368489583334
sparsity Attention k_chunks: 0.9659016927083334
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9644368489583334
sparsity Attention postReshape k_chunks: 0.9659016927083334
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9751790364583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9674479166666666
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6437174479166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672444661458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6410319010416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9658203125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.965576171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9662272135416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9658203125
sparsity Attention k_chunks: 0.965576171875
sparsity Attention v_chunks: 0.9662272135416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9658203125
sparsity Attention postReshape k_chunks: 0.965576171875
sparsity Attention postReshape v_chunks: 0.9662272135416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9754231770833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.978515625
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6424967447916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96600341796875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0467 seconds
Run 4 - Tempo totale del modello: 0.0467 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6783040364583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9711100260416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9705403645833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9711100260416666
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.9705403645833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9711100260416666
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.9705403645833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9793294270833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.68603515625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9709676106770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6702473958333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9810384114583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9886067708333334
sparsity Attention postReshape chunk - 1 output: 0.9736328125
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9820963541666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.670654296875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680379231770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.660888671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9762369791666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9703776041666666
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.660888671875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96856689453125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.658935546875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9661458333333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9664713541666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.9661458333333334
sparsity Attention v_chunks: 0.9664713541666666
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.9661458333333334
sparsity Attention postReshape v_chunks: 0.9664713541666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9723307291666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9742838541666666
sparsity Attention postReshape chunk - 2 output: 0.962890625
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.654296875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9662272135416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6486002604166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968017578125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.968017578125
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.968017578125
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9724934895833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9742838541666666
sparsity Attention postReshape chunk - 1 output: 0.9677734375
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9733072916666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9951171875
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6476236979166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9666748046875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.643798828125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9661458333333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9661458333333334
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9661458333333334
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9693196614583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.966796875
sparsity Attention postReshape chunk - 1 output: 0.9700520833333334
sparsity Attention postReshape chunk - 2 output: 0.9658203125
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9951171875
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6449381510416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9657999674479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6424967447916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966552734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966796875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966552734375
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.966796875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966552734375
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.966796875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9662272135416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9703776041666666
sparsity Attention postReshape chunk - 1 output: 0.96484375
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.958984375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.99609375
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.643310546875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9656982421875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.638427734375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9644368489583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9654947916666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.964599609375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9644368489583334
sparsity Attention k_chunks: 0.9654947916666666
sparsity Attention v_chunks: 0.964599609375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9644368489583334
sparsity Attention postReshape k_chunks: 0.9654947916666666
sparsity Attention postReshape v_chunks: 0.964599609375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9672037760416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9563802083333334
sparsity Attention postReshape chunk - 1 output: 0.9638671875
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6407877604166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9656778971354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0472 seconds
Run 5 - Tempo totale del modello: 0.0472 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6974283854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9691569010416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9725748697916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9691569010416666
sparsity Attention v_chunks: 0.9725748697916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9691569010416666
sparsity Attention postReshape v_chunks: 0.9725748697916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.980712890625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.99609375
sparsity Attention postReshape chunk - 1 output: 0.9889322916666666
sparsity Attention postReshape chunk - 2 output: 0.9661458333333334
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9928385416666666
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6920572916666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701131184895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6817220052083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9678548177083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.9678548177083334
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.9678548177083334
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.97265625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9912109375
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.958984375
sparsity Attention postReshape chunk - 3 output: 0.9606119791666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6822102864583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.970458984375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.673583984375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9717610677083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9706217447916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9683430989583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9717610677083334
sparsity Attention k_chunks: 0.9706217447916666
sparsity Attention v_chunks: 0.9683430989583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9717610677083334
sparsity Attention postReshape k_chunks: 0.9706217447916666
sparsity Attention postReshape v_chunks: 0.9683430989583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9970703125
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9680989583333334
sparsity Attention postReshape chunk - 3 output: 0.9580078125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6685384114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695027669270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6707356770833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698893229166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.9698893229166666
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.9698893229166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9742838541666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9886067708333334
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9680989583333334
sparsity Attention postReshape chunk - 3 output: 0.9619140625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.663818359375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9689737955729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.660400390625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9694010416666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.9694010416666666
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.9694010416666666
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.97705078125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6607259114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9697672526041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6559244791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9673665364583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.9673665364583334
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.9673665364583334
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.972900390625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9583333333333334
sparsity Attention postReshape chunk - 3 output: 0.9680989583333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.655517578125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681599934895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6542154947916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966552734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96630859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966552734375
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.96630859375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966552734375
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.96630859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9711100260416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9697265625
sparsity Attention postReshape chunk - 2 output: 0.9674479166666666
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9964192708333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6529947916666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96807861328125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6552734375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966064453125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966064453125
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966064453125
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9717610677083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.97265625
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9596354166666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6494954427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676920572916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
[Time] - Spikeformer Time time: 0.0472 seconds
Run 6 - Tempo totale del modello: 0.0472 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6851399739583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9718424479166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9713541666666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9716796875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9718424479166666
sparsity Attention k_chunks: 0.9713541666666666
sparsity Attention v_chunks: 0.9716796875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9718424479166666
sparsity Attention postReshape k_chunks: 0.9713541666666666
sparsity Attention postReshape v_chunks: 0.9716796875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9795735677083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9951171875
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6929524739583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9715169270833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.67578125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9701334635416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9701334635416666
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9701334635416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.97607421875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.966796875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9951171875
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6749674479166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9698689778645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.67041015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9774576822916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9879557291666666
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6685384114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9699300130208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6631673177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9700520833333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96826171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9700520833333334
sparsity Attention k_chunks: 0.96826171875
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9700520833333334
sparsity Attention postReshape k_chunks: 0.96826171875
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9764811197916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9895833333333334
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9671223958333334
sparsity Attention postReshape chunk - 3 output: 0.9664713541666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.994140625
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.658935546875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678141276041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6558430989583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9700520833333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.9700520833333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.9700520833333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.97607421875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.966796875
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9964192708333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6512044270833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679158528645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6470540364583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9672037760416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.9672037760416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.9672037760416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9747721354166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9661458333333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6476236979166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677734375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6490071614583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9683430989583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.9683430989583334
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.9683430989583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9775390625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9847005208333334
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.643798828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672444661458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.638671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9680989583333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96875
sparsity Attention k_chunks: 0.9680989583333334
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96875
sparsity Attention postReshape k_chunks: 0.9680989583333334
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9762369791666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.9847005208333334
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9654947916666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6381022135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9664306640625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0476 seconds
Run 7 - Tempo totale del modello: 0.0477 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6839192708333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9702962239583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9708658854166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9724934895833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9702962239583334
sparsity Attention k_chunks: 0.9708658854166666
sparsity Attention v_chunks: 0.9724934895833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9702962239583334
sparsity Attention postReshape k_chunks: 0.9708658854166666
sparsity Attention postReshape v_chunks: 0.9724934895833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.977783203125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9957682291666666
sparsity Attention postReshape chunk - 1 output: 0.9892578125
sparsity Attention postReshape chunk - 2 output: 0.9664713541666666
sparsity Attention postReshape chunk - 3 output: 0.9596354166666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6771647135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9708658854166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6688639322916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9710286458333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97265625
sparsity Attention k_chunks: 0.9710286458333334
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.97265625
sparsity Attention postReshape k_chunks: 0.9710286458333334
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9827473958333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9957682291666666
sparsity Attention postReshape chunk - 1 output: 0.9928385416666666
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.966796875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6682942708333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695027669270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6639811197916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9736328125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9944661458333334
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.951171875
sparsity Attention postReshape chunk - 3 output: 0.96484375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.663818359375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9693196614583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6630859375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9712727864583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9694010416666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9700520833333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9712727864583334
sparsity Attention k_chunks: 0.9694010416666666
sparsity Attention v_chunks: 0.9700520833333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9712727864583334
sparsity Attention postReshape k_chunks: 0.9694010416666666
sparsity Attention postReshape v_chunks: 0.9700520833333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9801432291666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9889322916666666
sparsity Attention postReshape chunk - 1 output: 0.9892578125
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9954427083333334
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.659423828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96856689453125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6529947916666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96923828125
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96923828125
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9790852864583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.9873046875
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.653564453125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685465494791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6539713541666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9810384114583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9873046875
sparsity Attention postReshape chunk - 1 output: 0.990234375
sparsity Attention postReshape chunk - 2 output: 0.9645182291666666
sparsity Attention postReshape chunk - 3 output: 0.9820963541666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6517740885416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690144856770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6459147135416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.97705078125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9801432291666666
sparsity Attention postReshape chunk - 3 output: 0.9641927083333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6442057291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96807861328125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.642333984375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698893229166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9680989583333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698893229166666
sparsity Attention k_chunks: 0.9680989583333334
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9698893229166666
sparsity Attention postReshape k_chunks: 0.9680989583333334
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9755045572916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9635416666666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6417643229166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685262044270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0472 seconds
Run 8 - Tempo totale del modello: 0.0472 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6752115885416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.972900390625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9717610677083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.972900390625
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.9717610677083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.972900390625
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.9717610677083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9808756510416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9938151041666666
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9772135416666666
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.67236328125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9697265625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.666748046875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9696451822916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.9696451822916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.9696451822916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9772135416666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9742838541666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6632486979166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695638020833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6612955729166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9658203125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96728515625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966552734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9658203125
sparsity Attention k_chunks: 0.96728515625
sparsity Attention v_chunks: 0.966552734375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9658203125
sparsity Attention postReshape k_chunks: 0.96728515625
sparsity Attention postReshape v_chunks: 0.966552734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.97216796875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9798177083333334
sparsity Attention postReshape chunk - 1 output: 0.9713541666666666
sparsity Attention postReshape chunk - 2 output: 0.9645182291666666
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6532389322916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678955078125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.647705078125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966552734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966552734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966552734375
sparsity Attention k_chunks: 0.966552734375
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966552734375
sparsity Attention postReshape k_chunks: 0.966552734375
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9739583333333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9720052083333334
sparsity Attention postReshape chunk - 1 output: 0.9710286458333334
sparsity Attention postReshape chunk - 2 output: 0.9772135416666666
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.99609375
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6490071614583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679972330729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6433919270833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9766438802083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9807942708333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.642578125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96734619140625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6394856770833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9658203125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9658203125
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9658203125
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9736328125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.9729817708333334
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6420084635416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680989583333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6366373697916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9645182291666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9654134114583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9645182291666666
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.9654134114583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9645182291666666
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.9654134114583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9684244791666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9720052083333334
sparsity Attention postReshape chunk - 1 output: 0.97265625
sparsity Attention postReshape chunk - 2 output: 0.9641927083333334
sparsity Attention postReshape chunk - 3 output: 0.96484375
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.636474609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672444661458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.63427734375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968505859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9640299479166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968505859375
sparsity Attention k_chunks: 0.9640299479166666
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968505859375
sparsity Attention postReshape k_chunks: 0.9640299479166666
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.973876953125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6326497395833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9663289388020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
[Time] - Spikeformer Time time: 0.0474 seconds
Run 9 - Tempo totale del modello: 0.0474 secondi
[Time] - SPS.PSM: 0.0016 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6813151041666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9701334635416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.9701334635416666
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.9701334635416666
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.97705078125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9625651041666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6787923177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9704996744791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6703287760416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9781901041666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.9876302083333334
sparsity Attention postReshape chunk - 2 output: 0.9772135416666666
sparsity Attention postReshape chunk - 3 output: 0.96484375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6644694010416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96868896484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6599934895833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9695638020833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9679361979166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9700520833333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9695638020833334
sparsity Attention k_chunks: 0.9679361979166666
sparsity Attention v_chunks: 0.9700520833333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9695638020833334
sparsity Attention postReshape k_chunks: 0.9679361979166666
sparsity Attention postReshape v_chunks: 0.9700520833333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9766438802083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6568196614583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682413736979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.65283203125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9671223958333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.9671223958333334
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.9671223958333334
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9778645833333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9817708333333334
sparsity Attention postReshape chunk - 3 output: 0.9674479166666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65283203125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670003255208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6527506510416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9677734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9677734375
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9677734375
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9755859375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9694010416666666
sparsity Attention postReshape chunk - 1 output: 0.9749348958333334
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9847005208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6551106770833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9661458333333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.650390625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9702962239583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9657389322916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9701334635416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9702962239583334
sparsity Attention k_chunks: 0.9657389322916666
sparsity Attention v_chunks: 0.9701334635416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9702962239583334
sparsity Attention postReshape k_chunks: 0.9657389322916666
sparsity Attention postReshape v_chunks: 0.9701334635416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9762369791666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.9736328125
sparsity Attention postReshape chunk - 3 output: 0.9697265625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9938151041666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6481119791666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96588134765625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6480305989583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9663899739583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9694010416666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.9663899739583334
sparsity Attention v_chunks: 0.9694010416666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.9663899739583334
sparsity Attention postReshape v_chunks: 0.9694010416666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9776204427083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.9716796875
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9847005208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6471354166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677734375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.645751953125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9662272135416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966064453125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9683430989583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9662272135416666
sparsity Attention k_chunks: 0.966064453125
sparsity Attention v_chunks: 0.9683430989583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9662272135416666
sparsity Attention postReshape k_chunks: 0.966064453125
sparsity Attention postReshape v_chunks: 0.9683430989583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9695638020833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9716796875
sparsity Attention postReshape chunk - 2 output: 0.9596354166666666
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9931640625
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.645751953125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9661051432291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0473 seconds
Run 10 - Tempo totale del modello: 0.0473 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.683349609375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970947265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9710286458333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97265625
sparsity Attention k_chunks: 0.970947265625
sparsity Attention v_chunks: 0.9710286458333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.97265625
sparsity Attention postReshape k_chunks: 0.970947265625
sparsity Attention postReshape v_chunks: 0.9710286458333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9830729166666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9873046875
sparsity Attention postReshape chunk - 1 output: 0.9869791666666666
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9811197916666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.68896484375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9696451822916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6744791666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.972900390625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.9690755208333334
sparsity Attention postReshape chunk - 3 output: 0.9615885416666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6758626302083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.969482421875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6690266927083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9680989583333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9661458333333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.9680989583333334
sparsity Attention v_chunks: 0.9661458333333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.9680989583333334
sparsity Attention postReshape v_chunks: 0.9661458333333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9730631510416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9723307291666666
sparsity Attention postReshape chunk - 2 output: 0.9661458333333334
sparsity Attention postReshape chunk - 3 output: 0.9677734375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6673177083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96771240234375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6637369791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970947265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.970947265625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.970947265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9794921875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.660888671875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96746826171875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6573079427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.979736328125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9899088541666666
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6520182291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674886067708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.648681640625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966552734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9654134114583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.966552734375
sparsity Attention v_chunks: 0.9654134114583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.966552734375
sparsity Attention postReshape v_chunks: 0.9654134114583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.974609375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.970703125
sparsity Attention postReshape chunk - 1 output: 0.9856770833333334
sparsity Attention postReshape chunk - 2 output: 0.9794921875
sparsity Attention postReshape chunk - 3 output: 0.9625651041666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6449381510416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96771240234375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.643798828125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9666341145833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9659830729166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.9666341145833334
sparsity Attention v_chunks: 0.9659830729166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.9666341145833334
sparsity Attention postReshape v_chunks: 0.9659830729166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9681803385416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9658203125
sparsity Attention postReshape chunk - 1 output: 0.9615885416666666
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.9664713541666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9918619791666666
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6395670572916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665730794270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6393229166666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9652506510416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9679361979166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966552734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9652506510416666
sparsity Attention k_chunks: 0.9679361979166666
sparsity Attention v_chunks: 0.966552734375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9652506510416666
sparsity Attention postReshape k_chunks: 0.9679361979166666
sparsity Attention postReshape v_chunks: 0.966552734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9737955729166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9684244791666666
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6409505208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96612548828125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0469 seconds
Run 11 - Tempo totale del modello: 0.0469 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0009 seconds
sparsity x_for_qkv: 0.6805013020833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.971923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9706217447916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.971923828125
sparsity Attention k_chunks: 0.9706217447916666
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.971923828125
sparsity Attention postReshape k_chunks: 0.9706217447916666
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.979736328125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.994140625
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6774088541666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9696248372395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.668212890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9696451822916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.9696451822916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.9696451822916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9732259114583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9612630208333334
sparsity Attention postReshape chunk - 3 output: 0.9638671875
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6595865885416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677327473958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6566569010416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9681803385416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9681803385416666
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9681803385416666
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.975341796875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9716796875
sparsity Attention postReshape chunk - 1 output: 0.9736328125
sparsity Attention postReshape chunk - 2 output: 0.9791666666666666
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6552734375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690755208333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6560872395833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9806315104166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9778645833333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6524251302083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686075846354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6471354166666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9668782552083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9663899739583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966552734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9668782552083334
sparsity Attention k_chunks: 0.9663899739583334
sparsity Attention v_chunks: 0.966552734375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9668782552083334
sparsity Attention postReshape k_chunks: 0.9663899739583334
sparsity Attention postReshape v_chunks: 0.966552734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9705403645833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9498697916666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6455891927083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671223958333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6429850260416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9651692708333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9638671875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9651692708333334
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.9638671875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9651692708333334
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.9638671875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9702962239583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.966796875
sparsity Attention postReshape chunk - 1 output: 0.9661458333333334
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9951171875
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.643798828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9668375651041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6427408854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9664713541666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9664713541666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9664713541666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9784342447916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6385091145833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673055013020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6383463541666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9647623697916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9647623697916666
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9647623697916666
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9729817708333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9811197916666666
sparsity Attention postReshape chunk - 1 output: 0.9576822916666666
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9772135416666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.634521484375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672037760416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
[Time] - Spikeformer Time time: 0.0478 seconds
Run 12 - Tempo totale del modello: 0.0478 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.684814453125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9702962239583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9715983072916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9702962239583334
sparsity Attention k_chunks: 0.97265625
sparsity Attention v_chunks: 0.9715983072916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9702962239583334
sparsity Attention postReshape k_chunks: 0.97265625
sparsity Attention postReshape v_chunks: 0.9715983072916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9796549479166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9944661458333334
sparsity Attention postReshape chunk - 1 output: 0.9850260416666666
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9664713541666666
[Time] - Transpose and LIF: 0.0005 seconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9889322916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6720377604166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.969970703125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.67138671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9717610677083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9694010416666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9717610677083334
sparsity Attention k_chunks: 0.9694010416666666
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9717610677083334
sparsity Attention postReshape k_chunks: 0.9694010416666666
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.978271484375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9778645833333334
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66943359375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690348307291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6661783854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96630859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.96630859375
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.96630859375
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.96923828125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.947265625
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9944661458333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6583658854166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677937825520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.655029296875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.969970703125
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.969970703125
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9775390625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9697265625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6542154947916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690348307291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6525065104166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9695638020833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9703776041666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9695638020833334
sparsity Attention k_chunks: 0.9703776041666666
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9695638020833334
sparsity Attention postReshape k_chunks: 0.9703776041666666
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9742838541666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9811197916666666
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9947916666666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6519368489583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682413736979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6482747395833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9736328125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.9733072916666666
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9638671875
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9947916666666666
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6488444010416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96826171875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.64404296875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9654134114583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96484375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9672037760416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9654134114583334
sparsity Attention k_chunks: 0.96484375
sparsity Attention v_chunks: 0.9672037760416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9654134114583334
sparsity Attention postReshape k_chunks: 0.96484375
sparsity Attention postReshape v_chunks: 0.9672037760416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9676920572916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9638671875
sparsity Attention postReshape chunk - 1 output: 0.9690755208333334
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9651692708333334
[Time] - Transpose and LIF: 0.0005 seconds
sparsity Attention postReshape chunk - 0 x: 0.9951171875
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6429850260416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681599934895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6410319010416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968994140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9671223958333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9662272135416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968994140625
sparsity Attention k_chunks: 0.9671223958333334
sparsity Attention v_chunks: 0.9662272135416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968994140625
sparsity Attention postReshape k_chunks: 0.9671223958333334
sparsity Attention postReshape v_chunks: 0.9662272135416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9747721354166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.9694010416666666
sparsity Attention postReshape chunk - 2 output: 0.9853515625
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.642578125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673868815104166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0475 seconds
Run 13 - Tempo totale del modello: 0.0475 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.68798828125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9720052083333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9700520833333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9720052083333334
sparsity Attention k_chunks: 0.9700520833333334
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9720052083333334
sparsity Attention postReshape k_chunks: 0.9700520833333334
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9803873697916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9899088541666666
sparsity Attention postReshape chunk - 1 output: 0.9918619791666666
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6995442708333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9696858723958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.684814453125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9686686197916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9686686197916666
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9686686197916666
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9764811197916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9847005208333334
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6778157552083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682820638020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.669677734375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9705403645833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96923828125
sparsity Attention k_chunks: 0.9705403645833334
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96923828125
sparsity Attention postReshape k_chunks: 0.9705403645833334
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.975830078125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9765625
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9765625
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6700846354166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695027669270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6666666666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968994140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968994140625
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968994140625
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9742838541666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9736328125
sparsity Attention postReshape chunk - 3 output: 0.9547526041666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6668294270833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686686197916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.66259765625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.976318359375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9716796875
sparsity Attention postReshape chunk - 2 output: 0.9765625
sparsity Attention postReshape chunk - 3 output: 0.9807942708333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.660888671875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680989583333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.65625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.9684244791666666
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9791666666666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.658203125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686075846354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6504720052083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.965087890625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9673665364583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9671223958333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.965087890625
sparsity Attention k_chunks: 0.9673665364583334
sparsity Attention v_chunks: 0.9671223958333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.965087890625
sparsity Attention postReshape k_chunks: 0.9673665364583334
sparsity Attention postReshape v_chunks: 0.9671223958333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9724934895833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9625651041666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9928385416666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6481119791666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9689737955729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.64453125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96826171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967529296875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.96826171875
sparsity Attention v_chunks: 0.967529296875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.96826171875
sparsity Attention postReshape v_chunks: 0.967529296875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9715169270833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9781901041666666
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.966796875
sparsity Attention postReshape chunk - 3 output: 0.9635416666666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6456705729166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96710205078125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0474 seconds
Run 14 - Tempo totale del modello: 0.0474 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6957194010416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9730631510416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970947265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.971923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9730631510416666
sparsity Attention k_chunks: 0.970947265625
sparsity Attention v_chunks: 0.971923828125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9730631510416666
sparsity Attention postReshape k_chunks: 0.970947265625
sparsity Attention postReshape v_chunks: 0.971923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.981689453125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9947916666666666
sparsity Attention postReshape chunk - 1 output: 0.986328125
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6874186197916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9716796875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6806640625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970458984375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.970458984375
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.970458984375
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9733072916666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9908854166666666
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.9560546875
sparsity Attention postReshape chunk - 3 output: 0.9625651041666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9954427083333334
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.68115234375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.970703125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.67333984375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9710286458333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9695638020833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9710286458333334
sparsity Attention k_chunks: 0.9695638020833334
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9710286458333334
sparsity Attention postReshape k_chunks: 0.9695638020833334
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9766438802083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9928385416666666
sparsity Attention postReshape chunk - 1 output: 0.9908854166666666
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9534505208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6743977864583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687906901041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6652018229166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9671223958333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.9671223958333334
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.9671223958333334
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9724934895833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.9765625
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.9583333333333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9954427083333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6604817708333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96868896484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6620279947916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9759928385416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9915364583333334
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.9615885416666666
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66015625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9692789713541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6591796875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97021484375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96728515625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97021484375
sparsity Attention k_chunks: 0.96728515625
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.97021484375
sparsity Attention postReshape k_chunks: 0.96728515625
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.970458984375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.953125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9954427083333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6548665364583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96795654296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6563313802083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97021484375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.97021484375
[Time] - Reshape: 0.0004 seconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.97021484375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.974365234375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9772135416666666
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9664713541666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6559244791666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677530924479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6487630208333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9771321614583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.986328125
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.647216796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679361979166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0470 seconds
Run 15 - Tempo totale del modello: 0.0470 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6787923177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698893229166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9718424479166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698893229166666
sparsity Attention k_chunks: 0.970703125
sparsity Attention v_chunks: 0.9718424479166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9698893229166666
sparsity Attention postReshape k_chunks: 0.970703125
sparsity Attention postReshape v_chunks: 0.9718424479166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9816080729166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9934895833333334
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9772135416666666
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6836751302083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9707234700520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6720377604166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9711100260416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9711100260416666
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9711100260416666
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.97412109375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98828125
sparsity Attention postReshape chunk - 1 output: 0.9847005208333334
sparsity Attention postReshape chunk - 2 output: 0.9658203125
sparsity Attention postReshape chunk - 3 output: 0.9576822916666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6643880208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9708251953125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6612955729166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9697265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9697265625
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9697265625
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9749348958333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9580078125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9951171875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.663330078125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688517252604166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6590169270833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9691569010416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966796875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96875
sparsity Attention k_chunks: 0.9691569010416666
sparsity Attention v_chunks: 0.966796875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96875
sparsity Attention postReshape k_chunks: 0.9691569010416666
sparsity Attention postReshape v_chunks: 0.966796875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.977294921875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.986328125
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.966796875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6561686197916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683634440104166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6529134114583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966796875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9657389322916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966796875
sparsity Attention k_chunks: 0.9657389322916666
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966796875
sparsity Attention postReshape k_chunks: 0.9657389322916666
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.97216796875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9781901041666666
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9583333333333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9938151041666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6516927083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96612548828125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6480305989583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9656575520833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96826171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9656575520833334
sparsity Attention k_chunks: 0.96826171875
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9656575520833334
sparsity Attention postReshape k_chunks: 0.96826171875
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.974853515625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.970703125
sparsity Attention postReshape chunk - 1 output: 0.9908854166666666
sparsity Attention postReshape chunk - 2 output: 0.9641927083333334
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6468098958333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96697998046875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6460774739583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9686686197916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9686686197916666
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9686686197916666
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9745279947916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.97265625
sparsity Attention postReshape chunk - 1 output: 0.9729817708333334
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.9801432291666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6433919270833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672648111979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6429036458333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9668782552083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9668782552083334
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9668782552083334
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9762369791666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9778645833333334
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6456705729166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9663289388020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0476 seconds
Run 16 - Tempo totale del modello: 0.0476 secondi
[Time] - SPS.PSM: 0.0016 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6829427083333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.971435546875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9707845052083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9695638020833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.971435546875
sparsity Attention k_chunks: 0.9707845052083334
sparsity Attention v_chunks: 0.9695638020833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.971435546875
sparsity Attention postReshape k_chunks: 0.9707845052083334
sparsity Attention postReshape v_chunks: 0.9695638020833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.97900390625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9873046875
sparsity Attention postReshape chunk - 1 output: 0.9908854166666666
sparsity Attention postReshape chunk - 2 output: 0.9736328125
sparsity Attention postReshape chunk - 3 output: 0.9641927083333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.673095703125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9705810546875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6639811197916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9715169270833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.973388671875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9717610677083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9715169270833334
sparsity Attention k_chunks: 0.973388671875
sparsity Attention v_chunks: 0.9717610677083334
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9715169270833334
sparsity Attention postReshape k_chunks: 0.973388671875
sparsity Attention postReshape v_chunks: 0.9717610677083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9825846354166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9921875
sparsity Attention postReshape chunk - 1 output: 0.9847005208333334
sparsity Attention postReshape chunk - 2 output: 0.96875
sparsity Attention postReshape chunk - 3 output: 0.9847005208333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6670735677083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96807861328125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.662841796875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676106770833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9676106770833334
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9676106770833334
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.975341796875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9847005208333334
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6605631510416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674479166666666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.65966796875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9663899739583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9663899739583334
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9663899739583334
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9705403645833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.97265625
sparsity Attention postReshape chunk - 1 output: 0.966796875
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.659912109375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669392903645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6510416666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9732259114583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9772135416666666
sparsity Attention postReshape chunk - 1 output: 0.9765625
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9944661458333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.653564453125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676310221354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6497395833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9671223958333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9671223958333334
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9671223958333334
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9745279947916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.966796875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6507161458333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678751627604166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6490885416666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9664713541666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9663899739583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9664713541666666
sparsity Attention k_chunks: 0.9663899739583334
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9664713541666666
sparsity Attention postReshape k_chunks: 0.9663899739583334
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.97412109375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9759114583333334
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9602864583333334
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6531575520833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675089518229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6476236979166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9769694010416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9811197916666666
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.643310546875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674275716145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0473 seconds
Run 17 - Tempo totale del modello: 0.0473 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6897786458333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97119140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9727376302083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.97119140625
sparsity Attention v_chunks: 0.9727376302083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.97119140625
sparsity Attention postReshape v_chunks: 0.9727376302083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9800618489583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9957682291666666
sparsity Attention postReshape chunk - 1 output: 0.9869791666666666
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9677734375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6842447916666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9708658854166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6822916666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9681803385416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97021484375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9681803385416666
sparsity Attention k_chunks: 0.97021484375
sparsity Attention v_chunks: 0.970703125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9681803385416666
sparsity Attention postReshape k_chunks: 0.97021484375
sparsity Attention postReshape v_chunks: 0.970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9749348958333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9944661458333334
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.953125
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9938151041666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6756998697916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695638020833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6685384114583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9768880208333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9934895833333334
sparsity Attention postReshape chunk - 1 output: 0.986328125
sparsity Attention postReshape chunk - 2 output: 0.9638671875
sparsity Attention postReshape chunk - 3 output: 0.9638671875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9925130208333334
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6649576822916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9691365559895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6580403645833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9697265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9697265625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9697265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9777018229166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9759114583333334
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.9742838541666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.662109375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.658203125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9666341145833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.9666341145833334
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.9666341145833334
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9711100260416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.9892578125
sparsity Attention postReshape chunk - 2 output: 0.9580078125
sparsity Attention postReshape chunk - 3 output: 0.9602864583333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6536458333333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671630859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6503092447916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9768880208333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.9814453125
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6468098958333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687093098958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6429850260416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9664713541666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.9664713541666666
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.9664713541666666
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9762369791666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9775390625
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9742838541666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6420084635416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686686197916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6398111979166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9673665364583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.9673665364583334
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.9673665364583334
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9732259114583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9635416666666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.637939453125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665323893229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
[Time] - Spikeformer Time time: 0.0505 seconds
Run 18 - Tempo totale del modello: 0.0505 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6918131510416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9718424479166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9706217447916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9705403645833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9718424479166666
sparsity Attention k_chunks: 0.9706217447916666
sparsity Attention v_chunks: 0.9705403645833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9718424479166666
sparsity Attention postReshape k_chunks: 0.9706217447916666
sparsity Attention postReshape v_chunks: 0.9705403645833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.982421875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9925130208333334
sparsity Attention postReshape chunk - 1 output: 0.9889322916666666
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6902669270833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9710489908854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6752115885416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9768880208333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9892578125
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9671223958333334
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.67626953125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695231119791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.668701171875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9707845052083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970458984375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9707845052083334
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.970458984375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9707845052083334
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.970458984375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.977783203125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9912109375
sparsity Attention postReshape chunk - 1 output: 0.9850260416666666
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9654947916666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6663411458333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672648111979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.662109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9725748697916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.974609375
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.966796875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9944661458333334
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6559244791666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682413736979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6549479166666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9661458333333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9661458333333334
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9661458333333334
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9740397135416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9615885416666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6500651041666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679361979166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6482747395833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.976806640625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.978515625
sparsity Attention postReshape chunk - 1 output: 0.9703776041666666
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9827473958333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.646484375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683837890625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6455078125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9668782552083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9656575520833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9668782552083334
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.9656575520833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9668782552083334
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.9656575520833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9755045572916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.647216796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96820068359375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.644775390625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.965576171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9652506510416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.965576171875
sparsity Attention k_chunks: 0.9652506510416666
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.965576171875
sparsity Attention postReshape k_chunks: 0.9652506510416666
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.974853515625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9684244791666666
sparsity Attention postReshape chunk - 1 output: 0.9892578125
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6455078125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682820638020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0512 seconds
Run 19 - Tempo totale del modello: 0.0512 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6775716145833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9702962239583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9712727864583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9707845052083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9702962239583334
sparsity Attention k_chunks: 0.9712727864583334
sparsity Attention v_chunks: 0.9707845052083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9702962239583334
sparsity Attention postReshape k_chunks: 0.9712727864583334
sparsity Attention postReshape v_chunks: 0.9707845052083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9822591145833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9918619791666666
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9801432291666666
sparsity Attention postReshape chunk - 3 output: 0.9788411458333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6798502604166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9711100260416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6683756510416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9697265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9713541666666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9697265625
sparsity Attention k_chunks: 0.9713541666666666
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 0.0004 seconds
sparsity Attention postReshape q_chunks: 0.9697265625
sparsity Attention postReshape k_chunks: 0.9713541666666666
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.977294921875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9899088541666666
sparsity Attention postReshape chunk - 1 output: 0.9752604166666666
sparsity Attention postReshape chunk - 2 output: 0.96875
sparsity Attention postReshape chunk - 3 output: 0.9752604166666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6658528645833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96942138671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0025 seconds
sparsity x_for_qkv: 0.66357421875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9695638020833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.9695638020833334
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.9695638020833334
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9789225260416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9733072916666666
[Time] - Transpose and LIF: 0.0005 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6664225260416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682820638020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6637369791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9693196614583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96923828125
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.9693196614583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96923828125
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.9693196614583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.978271484375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9801432291666666
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 0.0005 seconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0046 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.657470703125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672444661458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6543782552083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9659830729166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9671223958333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9659830729166666
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.9671223958333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9659830729166666
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.9671223958333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9746907552083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9654947916666666
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0046 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6524251302083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690958658854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0025 seconds
sparsity x_for_qkv: 0.6480305989583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968505859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968505859375
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968505859375
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9751790364583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9661458333333334
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0044 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.652587890625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.967529296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0030 seconds
sparsity x_for_qkv: 0.6483561197916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9666341145833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968017578125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9666341145833334
sparsity Attention k_chunks: 0.968017578125
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9666341145833334
sparsity Attention postReshape k_chunks: 0.968017578125
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9751790364583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9742838541666666
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.9775390625
sparsity Attention postReshape chunk - 3 output: 0.9622395833333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9964192708333334
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.647705078125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96685791015625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6451009114583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9666341145833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96533203125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967529296875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9666341145833334
sparsity Attention k_chunks: 0.96533203125
sparsity Attention v_chunks: 0.967529296875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9666341145833334
sparsity Attention postReshape k_chunks: 0.96533203125
sparsity Attention postReshape v_chunks: 0.967529296875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.978271484375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9964192708333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6443684895833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9655558268229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0554 seconds
Run 20 - Tempo totale del modello: 0.0554 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6806640625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9707845052083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.9707845052083334
[Time] - Reshape: 0.0004 seconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.9707845052083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9767252604166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.986328125
sparsity Attention postReshape chunk - 1 output: 0.9876302083333334
sparsity Attention postReshape chunk - 2 output: 0.9664713541666666
sparsity Attention postReshape chunk - 3 output: 0.9664713541666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6751302083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701131184895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6663411458333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9707845052083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9707845052083334
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 0.0002 seconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9707845052083334
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9813639322916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9860026041666666
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6615397135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9700927734375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6572265625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966796875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9671223958333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966796875
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9671223958333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966796875
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9671223958333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9759114583333334
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6537272135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9700724283854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6513671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.97509765625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.986328125
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9680989583333334
sparsity Attention postReshape chunk - 3 output: 0.9645182291666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9925130208333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6527506510416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690958658854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6516927083333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.974365234375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.96484375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65234375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9656168619791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6499837239583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9654134114583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9650065104166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9680989583333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9654134114583334
sparsity Attention k_chunks: 0.9650065104166666
sparsity Attention v_chunks: 0.9680989583333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9654134114583334
sparsity Attention postReshape k_chunks: 0.9650065104166666
sparsity Attention postReshape v_chunks: 0.9680989583333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.97119140625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9765625
sparsity Attention postReshape chunk - 1 output: 0.966796875
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9635416666666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.649169921875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687703450520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.644775390625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9657389322916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.9657389322916666
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.9657389322916666
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9729817708333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9703776041666666
sparsity Attention postReshape chunk - 3 output: 0.9602864583333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6460774739583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9661458333333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6427408854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9662272135416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96533203125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9662272135416666
sparsity Attention k_chunks: 0.96533203125
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9662272135416666
sparsity Attention postReshape k_chunks: 0.96533203125
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.969482421875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.978515625
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9527994791666666
sparsity Attention postReshape chunk - 3 output: 0.96484375
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6399739583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671630859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0518 seconds
Run 21 - Tempo totale del modello: 0.0518 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.684326171875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9680989583333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.9680989583333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.9680989583333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9801432291666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9889322916666666
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.679931640625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9693806966145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6719563802083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967529296875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967529296875
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.967529296875
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.975830078125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.966796875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6652018229166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679972330729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.661865234375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9733072916666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.97265625
sparsity Attention postReshape chunk - 2 output: 0.9641927083333334
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6565755208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674275716145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6505533854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967529296875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.967529296875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.967529296875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9749348958333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9576822916666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65478515625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9667765299479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6492513020833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9668782552083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9678548177083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9667154947916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9668782552083334
sparsity Attention k_chunks: 0.9678548177083334
sparsity Attention v_chunks: 0.9667154947916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9668782552083334
sparsity Attention postReshape k_chunks: 0.9678548177083334
sparsity Attention postReshape v_chunks: 0.9667154947916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9729817708333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9720052083333334
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6516927083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677734375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6459147135416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9662272135416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.9662272135416666
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.9662272135416666
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.97509765625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9697265625
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6459147135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676513671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6414388020833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9661458333333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9658203125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9666341145833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9661458333333334
sparsity Attention k_chunks: 0.9658203125
sparsity Attention v_chunks: 0.9666341145833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9661458333333334
sparsity Attention postReshape k_chunks: 0.9658203125
sparsity Attention postReshape v_chunks: 0.9666341145833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9672037760416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9694010416666666
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9518229166666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.99609375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9951171875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.643798828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677937825520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6385904947916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.975341796875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9651692708333334
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.982421875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6414388020833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9657185872395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
[Time] - Spikeformer Time time: 0.0516 seconds
Run 22 - Tempo totale del modello: 0.0516 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6909993489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9705403645833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9695638020833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9694010416666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9705403645833334
sparsity Attention k_chunks: 0.9695638020833334
sparsity Attention v_chunks: 0.9694010416666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9705403645833334
sparsity Attention postReshape k_chunks: 0.9695638020833334
sparsity Attention postReshape v_chunks: 0.9694010416666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9763997395833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9951171875
sparsity Attention postReshape chunk - 1 output: 0.9873046875
sparsity Attention postReshape chunk - 2 output: 0.9661458333333334
sparsity Attention postReshape chunk - 3 output: 0.95703125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9925130208333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6881510416666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96942138671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6810709635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9715983072916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.9715983072916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.9715983072916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9791666666666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9928385416666666
sparsity Attention postReshape chunk - 1 output: 0.9905598958333334
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 0.0005 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6846516927083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9684855143229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6747233072916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9744466145833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9915364583333334
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9622395833333334
sparsity Attention postReshape chunk - 3 output: 0.9635416666666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9918619791666666
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6748046875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96929931640625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6708984375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9786783854166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.670166015625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688313802083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.661376953125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9664713541666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9664713541666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9664713541666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9788411458333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9931640625
sparsity Attention postReshape chunk - 1 output: 0.9889322916666666
sparsity Attention postReshape chunk - 2 output: 0.9674479166666666
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6627604166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677937825520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6588541666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966552734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968017578125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967529296875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966552734375
sparsity Attention k_chunks: 0.968017578125
sparsity Attention v_chunks: 0.967529296875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966552734375
sparsity Attention postReshape k_chunks: 0.968017578125
sparsity Attention postReshape v_chunks: 0.967529296875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.974365234375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9886067708333334
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9671223958333334
sparsity Attention postReshape chunk - 3 output: 0.9645182291666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6553548177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9689737955729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.65283203125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9657389322916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9657389322916666
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9657389322916666
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9749348958333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9658203125
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6487630208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674886067708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.65380859375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9677734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9677734375
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9677734375
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9752604166666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9759114583333334
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6494954427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674479166666666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0511 seconds
Run 23 - Tempo totale del modello: 0.0511 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6810709635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9707845052083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.9707845052083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.9707845052083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9728190104166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9944661458333334
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.951171875
sparsity Attention postReshape chunk - 3 output: 0.9602864583333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9938151041666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6814778645833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9708658854166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6714680989583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97021484375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97021484375
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.97021484375
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9769694010416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.990234375
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9638671875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6697591145833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9692179361979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6654459635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9679361979166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968017578125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9651692708333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9679361979166666
sparsity Attention k_chunks: 0.968017578125
sparsity Attention v_chunks: 0.9651692708333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9679361979166666
sparsity Attention postReshape k_chunks: 0.968017578125
sparsity Attention postReshape v_chunks: 0.9651692708333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.974365234375
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.9856770833333334
sparsity Attention postReshape chunk - 2 output: 0.9632161458333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.662841796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680582682291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6637369791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9681803385416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9672037760416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9681803385416666
sparsity Attention k_chunks: 0.9672037760416666
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9681803385416666
sparsity Attention postReshape k_chunks: 0.9672037760416666
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9750162760416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9951171875
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6598307291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688313802083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.654052734375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9659830729166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9659830729166666
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9659830729166666
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9740397135416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9671223958333334
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6522623697916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679768880208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6500651041666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9664713541666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.9664713541666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.9664713541666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9722493489583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.9752604166666666
sparsity Attention postReshape chunk - 2 output: 0.9645182291666666
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6494954427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675496419270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6493326822916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9765625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.986328125
sparsity Attention postReshape chunk - 1 output: 0.9703776041666666
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9957682291666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.647216796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676920572916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.64404296875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968017578125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9662272135416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.968017578125
sparsity Attention v_chunks: 0.9662272135416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.968017578125
sparsity Attention postReshape v_chunks: 0.9662272135416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.973388671875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9723307291666666
sparsity Attention postReshape chunk - 1 output: 0.9658203125
sparsity Attention postReshape chunk - 2 output: 0.9775390625
sparsity Attention postReshape chunk - 3 output: 0.9778645833333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6471354166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669189453125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
[Time] - Spikeformer Time time: 0.0512 seconds
Run 24 - Tempo totale del modello: 0.0512 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6814778645833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9703776041666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9703776041666666
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9703776041666666
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9778645833333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0044 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6845703125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96783447265625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6700846354166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.977294921875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9765625
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6615397135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676513671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6598307291666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966552734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.966552734375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.966552734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9724934895833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9860026041666666
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9453125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6547037760416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680582682291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.64990234375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9662272135416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9657389322916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9662272135416666
sparsity Attention k_chunks: 0.9657389322916666
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9662272135416666
sparsity Attention postReshape k_chunks: 0.9657389322916666
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.971923828125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9749348958333334
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6520182291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9666748046875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6494954427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9686686197916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9686686197916666
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9686686197916666
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9785970052083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9759114583333334
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9820963541666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6456705729166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.967041015625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.64697265625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9793294270833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.984375
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.9733072916666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.650390625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96771240234375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.64599609375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9671223958333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.9671223958333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.9671223958333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9759928385416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9811197916666666
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9944661458333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6442057291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670613606770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6392415364583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9793294270833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.9736328125
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9833984375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6410319010416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672648111979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0521 seconds
Run 25 - Tempo totale del modello: 0.0522 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6735026041666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9703776041666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9700520833333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9703776041666666
sparsity Attention k_chunks: 0.9700520833333334
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9703776041666666
sparsity Attention postReshape k_chunks: 0.9700520833333334
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9755045572916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.9697265625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6744791666666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9708658854166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6630045572916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9700520833333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9700520833333334
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9700520833333334
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9828287760416666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9879557291666666
sparsity Attention postReshape chunk - 1 output: 0.9869791666666666
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6622721354166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9684855143229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6586100260416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96875
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 0.0004 seconds
sparsity Attention postReshape q_chunks: 0.96875
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.977294921875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9742838541666666
sparsity Attention postReshape chunk - 1 output: 0.9879557291666666
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6563313802083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686686197916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6519368489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9700520833333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9680989583333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9700520833333334
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9680989583333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9700520833333334
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9680989583333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9781087239583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.978515625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6498209635416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686075846354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6482747395833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966552734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966552734375
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966552734375
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9783528645833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.9788411458333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6483561197916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9667561848958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.647216796875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9666341145833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966552734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.9666341145833334
sparsity Attention v_chunks: 0.966552734375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.9666341145833334
sparsity Attention postReshape v_chunks: 0.966552734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9752604166666666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.9765625
sparsity Attention postReshape chunk - 2 output: 0.9654947916666666
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6477864583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678141276041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6449381510416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966796875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9688313802083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966796875
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9688313802083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966796875
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9688313802083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9728190104166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9661458333333334
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6431477864583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96807861328125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6416829427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9681803385416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9667154947916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9681803385416666
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9667154947916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9681803385416666
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9667154947916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.974853515625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9765625
sparsity Attention postReshape chunk - 1 output: 0.9749348958333334
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.978515625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.641357421875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669596354166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0518 seconds
Run 26 - Tempo totale del modello: 0.0518 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6925455729166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9710286458333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9710286458333334
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9710286458333334
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9746907552083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9912109375
sparsity Attention postReshape chunk - 1 output: 0.9856770833333334
sparsity Attention postReshape chunk - 2 output: 0.9651692708333334
sparsity Attention postReshape chunk - 3 output: 0.9567057291666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6876627604166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9713134765625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.676513671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9712727864583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.9712727864583334
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 0.0004 seconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.9712727864583334
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.978515625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9918619791666666
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9599609375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6786295572916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701334635416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6744791666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9697265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968017578125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9697265625
sparsity Attention k_chunks: 0.968017578125
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9697265625
sparsity Attention postReshape k_chunks: 0.968017578125
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9767252604166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9801432291666666
sparsity Attention postReshape chunk - 3 output: 0.9612630208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.671630859375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96868896484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6643880208333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9702962239583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966064453125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9702962239583334
sparsity Attention k_chunks: 0.966064453125
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9702962239583334
sparsity Attention postReshape k_chunks: 0.966064453125
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9774576822916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9651692708333334
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6626790364583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677734375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6582845052083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9695638020833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9673665364583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9695638020833334
sparsity Attention k_chunks: 0.9673665364583334
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9695638020833334
sparsity Attention postReshape k_chunks: 0.9673665364583334
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.979736328125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.986328125
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 0.0005 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0045 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6595052083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96856689453125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6570638020833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9700520833333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9703776041666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9664713541666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9700520833333334
sparsity Attention k_chunks: 0.9703776041666666
sparsity Attention v_chunks: 0.9664713541666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9700520833333334
sparsity Attention postReshape k_chunks: 0.9703776041666666
sparsity Attention postReshape v_chunks: 0.9664713541666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9793294270833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.984375
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9833984375
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6543782552083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685262044270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6509602864583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9736328125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.9749348958333334
sparsity Attention postReshape chunk - 2 output: 0.966796875
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9964192708333334
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6494954427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9662272135416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.644775390625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966796875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9679361979166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9683430989583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966796875
sparsity Attention k_chunks: 0.9679361979166666
sparsity Attention v_chunks: 0.9683430989583334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.966796875
sparsity Attention postReshape k_chunks: 0.9679361979166666
sparsity Attention postReshape v_chunks: 0.9683430989583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9745279947916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.974609375
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.643798828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676513671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
[Time] - Spikeformer Time time: 0.0523 seconds
Run 27 - Tempo totale del modello: 0.0523 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.68310546875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9737141927083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970947265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9737141927083334
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.970947265625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9737141927083334
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.970947265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9825032552083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9912109375
sparsity Attention postReshape chunk - 1 output: 0.984375
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9801432291666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6996256510416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9713338216145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.685302734375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9718424479166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9703776041666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9718424479166666
sparsity Attention k_chunks: 0.9703776041666666
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9718424479166666
sparsity Attention postReshape k_chunks: 0.9703776041666666
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9784342447916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9814453125
sparsity Attention postReshape chunk - 3 output: 0.9651692708333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.68798828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690348307291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6761881510416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9761555989583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9811197916666666
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.9817708333333334
sparsity Attention postReshape chunk - 3 output: 0.9677734375
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.668212890625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688313802083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6634928385416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9674479166666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9680989583333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9674479166666666
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.9680989583333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9674479166666666
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.9680989583333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9690755208333334
sparsity Attention postReshape chunk - 3 output: 0.9759114583333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6627604166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669392903645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6575520833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9666341145833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.9666341145833334
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.9666341145833334
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9751790364583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9798177083333334
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.962890625
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6538899739583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665120442708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.64990234375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9700520833333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9696451822916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9700520833333334
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.9696451822916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9700520833333334
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.9696451822916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9783528645833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9703776041666666
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9807942708333334
sparsity Attention postReshape chunk - 3 output: 0.9850260416666666
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6487630208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670817057291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6468098958333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96826171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9641927083333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.96826171875
sparsity Attention v_chunks: 0.9641927083333334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.96826171875
sparsity Attention postReshape v_chunks: 0.9641927083333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.97607421875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9671223958333334
sparsity Attention postReshape chunk - 2 output: 0.9847005208333334
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.646728515625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665730794270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6486002604166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96533203125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96533203125
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.96533203125
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.977294921875
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9759114583333334
sparsity Attention postReshape chunk - 1 output: 0.9684244791666666
sparsity Attention postReshape chunk - 2 output: 0.982421875
sparsity Attention postReshape chunk - 3 output: 0.982421875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.650146484375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96661376953125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0517 seconds
Run 28 - Tempo totale del modello: 0.0517 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0005 seconds
sparsity x_for_qkv: 0.6787923177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9701334635416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9701334635416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.971435546875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9701334635416666
sparsity Attention k_chunks: 0.9701334635416666
sparsity Attention v_chunks: 0.971435546875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9701334635416666
sparsity Attention postReshape k_chunks: 0.9701334635416666
sparsity Attention postReshape v_chunks: 0.971435546875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9781087239583334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9938151041666666
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9661458333333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6654459635416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9703572591145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.66064453125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97119140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9711100260416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.97119140625
sparsity Attention v_chunks: 0.9711100260416666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.97119140625
sparsity Attention postReshape v_chunks: 0.9711100260416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9813639322916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.98046875
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6578776041666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9702555338541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.655517578125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9697265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.9697265625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.9697265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9823404947916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.9856770833333334
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6544596354166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96868896484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6461588541666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9664713541666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.9664713541666666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.9664713541666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9727376302083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9736328125
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0045 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64453125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676310221354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6463216145833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9666341145833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9666341145833334
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9666341145833334
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.980712890625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.98046875
sparsity Attention postReshape chunk - 3 output: 0.9830729166666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6485188802083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678955078125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6441243489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9664713541666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96826171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9658203125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9664713541666666
sparsity Attention k_chunks: 0.96826171875
sparsity Attention v_chunks: 0.9658203125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9664713541666666
sparsity Attention postReshape k_chunks: 0.96826171875
sparsity Attention postReshape v_chunks: 0.9658203125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.96923828125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.97265625
sparsity Attention postReshape chunk - 1 output: 0.9697265625
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9661458333333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9964192708333334
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.9951171875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6397298177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9662679036458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6397298177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9645182291666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9645182291666666
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 0.0004 seconds
sparsity Attention postReshape q_chunks: 0.9645182291666666
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.9715169270833334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.9700520833333334
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.1241 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6400553385416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.967041015625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0025 seconds
sparsity x_for_qkv: 0.6385091145833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 0.0004 seconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9775390625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9775390625
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9697265625
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.635986328125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9684855143229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.1736 seconds
Run 29 - Tempo totale del modello: 0.1736 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.677001953125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698893229166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.971435546875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698893229166666
sparsity Attention k_chunks: 0.971435546875
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9698893229166666
sparsity Attention postReshape k_chunks: 0.971435546875
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.978515625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 0.0003 seconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.67626953125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9698689778645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6631673177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9680989583333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9680989583333334
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9680989583333334
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.978515625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9954427083333334
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65771484375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9691365559895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6586100260416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9805501302083334
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9895833333333334
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9781901041666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.650390625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96923828125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6512044270833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.97509765625
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9759114583333334
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9674479166666666
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6525065104166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680379231770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.642822265625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.977783203125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9889322916666666
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9664713541666666
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6429036458333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678955078125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6453450520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9707845052083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9707845052083334
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9707845052083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0001 seconds
sparsity Attention out: 0.97705078125
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9781901041666666
sparsity Attention postReshape chunk - 1 output: 0.9791666666666666
sparsity Attention postReshape chunk - 2 output: 0.9847005208333334
sparsity Attention postReshape chunk - 3 output: 0.9661458333333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9918619791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6438802083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96734619140625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6415201822916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676106770833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9676106770833334
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9676106770833334
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0003 seconds
sparsity Attention out: 0.9755045572916666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 0.0005 seconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6387532552083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672444661458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6378580729166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9681803385416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9659830729166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9681803385416666
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9659830729166666
[Time] - Reshape: 0.0003 seconds
sparsity Attention postReshape q_chunks: 0.9681803385416666
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9659830729166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 0.0002 seconds
sparsity Attention out: 0.9747721354166666
[Time] - Reshape Time-Space: 0.0000 seconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.970703125
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 0.0004 seconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6411946614583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671427408854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0524 seconds
Run 30 - Tempo totale del modello: 0.0524 secondi

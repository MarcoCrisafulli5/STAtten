[Time] - SPS.PSM: 0.0019 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6822916666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97021484375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.97021484375
[Time] - Reshape: 295692.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.97021484375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 168754.0000 nanoseconds
sparsity Attention out: 0.979736328125
[Time] - Reshape Time-Space: 25699.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.9791666666666666
sparsity Attention postReshape chunk - 3 output: 0.9794921875
[Time] - Transpose and LIF: 356262.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6841634114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9723917643229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.67529296875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9711100260416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9708658854166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9712727864583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9711100260416666
sparsity Attention k_chunks: 0.9708658854166666
sparsity Attention v_chunks: 0.9712727864583334
[Time] - Reshape: 282436.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9711100260416666
sparsity Attention postReshape k_chunks: 0.9708658854166666
sparsity Attention postReshape v_chunks: 0.9712727864583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 125170.0000 nanoseconds
sparsity Attention out: 0.9837239583333334
[Time] - Reshape Time-Space: 22208.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9820963541666666
sparsity Attention postReshape chunk - 3 output: 0.9853515625
[Time] - Transpose and LIF: 318325.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.673828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9692179361979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6654459635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9695638020833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9695638020833334
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 284593.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9695638020833334
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 117748.0000 nanoseconds
sparsity Attention out: 0.9808756510416666
[Time] - Reshape Time-Space: 21847.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.984375
[Time] - Transpose and LIF: 324371.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6644694010416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9697469075520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6575520833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9695638020833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969482421875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.9695638020833334
sparsity Attention v_chunks: 0.969482421875
[Time] - Reshape: 286547.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.9695638020833334
sparsity Attention postReshape v_chunks: 0.969482421875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 115259.0000 nanoseconds
sparsity Attention out: 0.9807942708333334
[Time] - Reshape Time-Space: 21298.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9837239583333334
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 309771.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6548665364583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.967529296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6503092447916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9680989583333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9700520833333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.9680989583333334
sparsity Attention v_chunks: 0.9700520833333334
[Time] - Reshape: 284417.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.9680989583333334
sparsity Attention postReshape v_chunks: 0.9700520833333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 120194.0000 nanoseconds
sparsity Attention out: 0.9793294270833334
[Time] - Reshape Time-Space: 22106.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9892578125
sparsity Attention postReshape chunk - 1 output: 0.9716796875
sparsity Attention postReshape chunk - 2 output: 0.9794921875
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 323892.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6536458333333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9694417317708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.650146484375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96630859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96826171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96630859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96630859375
sparsity Attention k_chunks: 0.96826171875
sparsity Attention v_chunks: 0.96630859375
[Time] - Reshape: 287452.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96630859375
sparsity Attention postReshape k_chunks: 0.96826171875
sparsity Attention postReshape v_chunks: 0.96630859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 113415.0000 nanoseconds
sparsity Attention out: 0.9711100260416666
[Time] - Reshape Time-Space: 21046.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.966796875
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9602864583333334
[Time] - Transpose and LIF: 315459.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9947916666666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6521809895833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676717122395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6536458333333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 284742.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 112384.0000 nanoseconds
sparsity Attention out: 0.9730631510416666
[Time] - Reshape Time-Space: 21702.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9680989583333334
sparsity Attention postReshape chunk - 1 output: 0.9765625
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.9798177083333334
[Time] - Transpose and LIF: 326165.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6513671875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676717122395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6451822916666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966796875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96630859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966796875
sparsity Attention k_chunks: 0.96630859375
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 276533.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.966796875
sparsity Attention postReshape k_chunks: 0.96630859375
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108902.0000 nanoseconds
sparsity Attention out: 0.9697265625
[Time] - Reshape Time-Space: 21105.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9654947916666666
sparsity Attention postReshape chunk - 1 output: 0.9612630208333334
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 323846.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9944661458333334
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6446940104166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96661376953125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
[Time] - Spikeformer Time time: 0.0456 seconds
Run 1 - Tempo totale del modello: 0.0456 secondi
[Time] - SPS.PSM: 0.0014 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.68017578125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9711100260416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9715983072916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9703776041666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9711100260416666
sparsity Attention k_chunks: 0.9715983072916666
sparsity Attention v_chunks: 0.9703776041666666
[Time] - Reshape: 277107.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9711100260416666
sparsity Attention postReshape k_chunks: 0.9715983072916666
sparsity Attention postReshape v_chunks: 0.9703776041666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108397.0000 nanoseconds
sparsity Attention out: 0.9812825520833334
[Time] - Reshape Time-Space: 20862.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9947916666666666
sparsity Attention postReshape chunk - 1 output: 0.9850260416666666
sparsity Attention postReshape chunk - 2 output: 0.9772135416666666
sparsity Attention postReshape chunk - 3 output: 0.9680989583333334
[Time] - Transpose and LIF: 311429.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6791178385416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9705607096354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.670166015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9679361979166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9706217447916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9679361979166666
sparsity Attention k_chunks: 0.9706217447916666
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 281156.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9679361979166666
sparsity Attention postReshape k_chunks: 0.9706217447916666
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106502.0000 nanoseconds
sparsity Attention out: 0.97802734375
[Time] - Reshape Time-Space: 19891.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9801432291666666
[Time] - Transpose and LIF: 310819.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6649576822916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9696248372395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6609700520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970458984375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.970458984375
[Time] - Reshape: 277164.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.970458984375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106903.0000 nanoseconds
sparsity Attention out: 0.9761555989583334
[Time] - Reshape Time-Space: 21058.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9690755208333334
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9788411458333334
[Time] - Transpose and LIF: 318854.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66064453125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673665364583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.653076171875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 279310.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106102.0000 nanoseconds
sparsity Attention out: 0.98046875
[Time] - Reshape Time-Space: 19851.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.9791666666666666
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9781901041666666
[Time] - Transpose and LIF: 317071.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9934895833333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.654296875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.968017578125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.6527506510416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968505859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966552734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968505859375
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.966552734375
[Time] - Reshape: 276976.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968505859375
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.966552734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107530.0000 nanoseconds
sparsity Attention out: 0.974853515625
[Time] - Reshape Time-Space: 19744.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9733072916666666
sparsity Attention postReshape chunk - 1 output: 0.97265625
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 326198.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.99609375
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6555989583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679972330729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.650146484375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 276832.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105818.0000 nanoseconds
sparsity Attention out: 0.979736328125
[Time] - Reshape Time-Space: 21464.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.982421875
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 315738.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9964192708333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6481119791666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96630859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.6468098958333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9666341145833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9666341145833334
[Time] - Reshape: 276135.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9666341145833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104976.0000 nanoseconds
sparsity Attention out: 0.9696451822916666
[Time] - Reshape Time-Space: 20306.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9729817708333334
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9554036458333334
[Time] - Transpose and LIF: 316387.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.642578125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669596354166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6443684895833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9668782552083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9651692708333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9668782552083334
sparsity Attention k_chunks: 0.9651692708333334
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 280108.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9668782552083334
sparsity Attention postReshape k_chunks: 0.9651692708333334
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105546.0000 nanoseconds
sparsity Attention out: 0.9750162760416666
[Time] - Reshape Time-Space: 21541.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9612630208333334
[Time] - Transpose and LIF: 317168.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9954427083333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.640625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9656575520833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
[Time] - Spikeformer Time time: 0.0435 seconds
Run 2 - Tempo totale del modello: 0.0435 secondi
[Time] - SPS.PSM: 0.0028 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6809895833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9705403645833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.971435546875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9715169270833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9705403645833334
sparsity Attention k_chunks: 0.971435546875
sparsity Attention v_chunks: 0.9715169270833334
[Time] - Reshape: 278370.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9705403645833334
sparsity Attention postReshape k_chunks: 0.971435546875
sparsity Attention postReshape v_chunks: 0.9715169270833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 261845.0000 nanoseconds
sparsity Attention out: 0.9822591145833334
[Time] - Reshape Time-Space: 27278.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.986328125
sparsity Attention postReshape chunk - 3 output: 0.9752604166666666
[Time] - Transpose and LIF: 371121.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6675618489583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9700113932291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.65966796875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9701334635416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9711100260416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9701334635416666
sparsity Attention k_chunks: 0.9711100260416666
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 276705.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9701334635416666
sparsity Attention postReshape k_chunks: 0.9711100260416666
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260201.0000 nanoseconds
sparsity Attention out: 0.9789225260416666
[Time] - Reshape Time-Space: 27715.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9820963541666666
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 383831.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6551106770833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9694620768229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6500651041666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698893229166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698893229166666
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 267304.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698893229166666
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 163534.0000 nanoseconds
sparsity Attention out: 0.9816080729166666
[Time] - Reshape Time-Space: 21603.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9830729166666666
sparsity Attention postReshape chunk - 3 output: 0.9794921875
[Time] - Transpose and LIF: 378357.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0064 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6509602864583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9691365559895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0030 seconds
sparsity x_for_qkv: 0.6466471354166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 276841.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 271006.0000 nanoseconds
sparsity Attention out: 0.9817708333333334
[Time] - Reshape Time-Space: 28121.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.98046875
sparsity Attention postReshape chunk - 3 output: 0.9801432291666666
[Time] - Transpose and LIF: 368049.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6446126302083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685262044270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6420084635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9703776041666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9694010416666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9703776041666666
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9694010416666666
[Time] - Reshape: 276395.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9703776041666666
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9694010416666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253296.0000 nanoseconds
sparsity Attention out: 0.978271484375
[Time] - Reshape Time-Space: 28263.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.982421875
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 382531.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6451009114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681396484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.64013671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9700520833333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9700520833333334
[Time] - Reshape: 256571.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9700520833333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 113725.0000 nanoseconds
sparsity Attention out: 0.9752604166666666
[Time] - Reshape Time-Space: 21447.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9716796875
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 364506.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6404622395833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96807861328125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6366373697916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9680989583333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.9680989583333334
[Time] - Reshape: 256567.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.9680989583333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 269764.0000 nanoseconds
sparsity Attention out: 0.97998046875
[Time] - Reshape Time-Space: 28423.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9781901041666666
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9807942708333334
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 379398.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9964192708333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6380208333333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686686197916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.636474609375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9669596354166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9669596354166666
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 262862.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9669596354166666
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 251115.0000 nanoseconds
sparsity Attention out: 0.97314453125
[Time] - Reshape Time-Space: 28334.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9697265625
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9697265625
[Time] - Transpose and LIF: 381537.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.635986328125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677530924479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
[Time] - Spikeformer Time time: 0.0535 seconds
Run 3 - Tempo totale del modello: 0.0535 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6765950520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.972412109375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9696451822916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.972412109375
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.9696451822916666
[Time] - Reshape: 252525.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.972412109375
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.9696451822916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 254962.0000 nanoseconds
sparsity Attention out: 0.9815266927083334
[Time] - Reshape Time-Space: 29492.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9847005208333334
sparsity Attention postReshape chunk - 1 output: 0.9918619791666666
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 366367.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6883138020833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695027669270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6697591145833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970947265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9696451822916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970947265625
sparsity Attention k_chunks: 0.9696451822916666
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 264004.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970947265625
sparsity Attention postReshape k_chunks: 0.9696451822916666
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107536.0000 nanoseconds
sparsity Attention out: 0.97900390625
[Time] - Reshape Time-Space: 21683.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.986328125
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.9820963541666666
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 382120.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6687825520833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9691365559895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6625162760416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966552734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.966552734375
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 255114.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.966552734375
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258353.0000 nanoseconds
sparsity Attention out: 0.9768880208333334
[Time] - Reshape Time-Space: 30040.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.9697265625
sparsity Attention postReshape chunk - 2 output: 0.9853515625
sparsity Attention postReshape chunk - 3 output: 0.9733072916666666
[Time] - Transpose and LIF: 377975.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.660888671875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.654541015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676106770833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969482421875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96875
sparsity Attention k_chunks: 0.9676106770833334
sparsity Attention v_chunks: 0.969482421875
[Time] - Reshape: 258361.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96875
sparsity Attention postReshape k_chunks: 0.9676106770833334
sparsity Attention postReshape v_chunks: 0.969482421875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264264.0000 nanoseconds
sparsity Attention out: 0.97607421875
[Time] - Reshape Time-Space: 28714.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9775390625
sparsity Attention postReshape chunk - 1 output: 0.9736328125
sparsity Attention postReshape chunk - 2 output: 0.9736328125
sparsity Attention postReshape chunk - 3 output: 0.9794921875
[Time] - Transpose and LIF: 374331.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6571451822916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690348307291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6529947916666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96728515625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.96728515625
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 258677.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.96728515625
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107515.0000 nanoseconds
sparsity Attention out: 0.9746907552083334
[Time] - Reshape Time-Space: 20432.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9794921875
sparsity Attention postReshape chunk - 1 output: 0.9654947916666666
sparsity Attention postReshape chunk - 2 output: 0.984375
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 369098.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6490071614583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679158528645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.64697265625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9700520833333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9680989583333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9700520833333334
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.9680989583333334
[Time] - Reshape: 253047.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9700520833333334
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.9680989583333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256301.0000 nanoseconds
sparsity Attention out: 0.9757486979166666
[Time] - Reshape Time-Space: 27822.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9703776041666666
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9794921875
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 364164.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.647216796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96820068359375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.64599609375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96630859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9672037760416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9671223958333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96630859375
sparsity Attention k_chunks: 0.9672037760416666
sparsity Attention v_chunks: 0.9671223958333334
[Time] - Reshape: 257044.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96630859375
sparsity Attention postReshape k_chunks: 0.9672037760416666
sparsity Attention postReshape v_chunks: 0.9671223958333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253082.0000 nanoseconds
sparsity Attention out: 0.974609375
[Time] - Reshape Time-Space: 27933.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.970703125
sparsity Attention postReshape chunk - 1 output: 0.9733072916666666
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 452876.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.643798828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680989583333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6414388020833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966552734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9657389322916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.966552734375
sparsity Attention v_chunks: 0.9657389322916666
[Time] - Reshape: 252997.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.966552734375
sparsity Attention postReshape v_chunks: 0.9657389322916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 150543.0000 nanoseconds
sparsity Attention out: 0.9715169270833334
[Time] - Reshape Time-Space: 23338.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9716796875
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.9680989583333334
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 376755.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.643798828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686686197916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0483 seconds
Run 4 - Tempo totale del modello: 0.0484 secondi
[Time] - SPS.PSM: 0.0019 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6787109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9700520833333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.971435546875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.9700520833333334
sparsity Attention v_chunks: 0.971435546875
[Time] - Reshape: 257636.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.9700520833333334
sparsity Attention postReshape v_chunks: 0.971435546875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107410.0000 nanoseconds
sparsity Attention out: 0.9763997395833334
[Time] - Reshape Time-Space: 21936.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9635416666666666
[Time] - Transpose and LIF: 372194.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6726888020833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96954345703125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6629231770833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9707845052083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9710286458333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9696451822916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9707845052083334
sparsity Attention k_chunks: 0.9710286458333334
sparsity Attention v_chunks: 0.9696451822916666
[Time] - Reshape: 261497.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9707845052083334
sparsity Attention postReshape k_chunks: 0.9710286458333334
sparsity Attention postReshape v_chunks: 0.9696451822916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260704.0000 nanoseconds
sparsity Attention out: 0.9814453125
[Time] - Reshape Time-Space: 28678.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9934895833333334
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9775390625
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 368612.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6644694010416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690958658854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6603190104166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9700520833333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9688313802083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9700520833333334
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.9688313802083334
[Time] - Reshape: 256243.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9700520833333334
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.9688313802083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256520.0000 nanoseconds
sparsity Attention out: 0.9769694010416666
[Time] - Reshape Time-Space: 28298.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9850260416666666
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9664713541666666
[Time] - Transpose and LIF: 369670.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6541341145833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96990966796875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.64892578125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966552734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.966552734375
[Time] - Reshape: 257930.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.966552734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110124.0000 nanoseconds
sparsity Attention out: 0.972900390625
[Time] - Reshape Time-Space: 28397.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.974609375
sparsity Attention postReshape chunk - 1 output: 0.9680989583333334
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9742838541666666
[Time] - Transpose and LIF: 371573.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6487630208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9684855143229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.643798828125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9651692708333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.9651692708333334
[Time] - Reshape: 253212.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.9651692708333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264152.0000 nanoseconds
sparsity Attention out: 0.971923828125
[Time] - Reshape Time-Space: 27068.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9716796875
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9674479166666666
sparsity Attention postReshape chunk - 3 output: 0.9664713541666666
[Time] - Transpose and LIF: 383163.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6453450520833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670817057291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.643798828125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97119140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9679361979166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97119140625
sparsity Attention k_chunks: 0.9679361979166666
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 252797.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.97119140625
sparsity Attention postReshape k_chunks: 0.9679361979166666
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 250819.0000 nanoseconds
sparsity Attention out: 0.9795735677083334
[Time] - Reshape Time-Space: 27514.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9840494791666666
sparsity Attention postReshape chunk - 3 output: 0.9772135416666666
[Time] - Transpose and LIF: 361866.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6487630208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681803385416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6427408854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9672037760416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.9672037760416666
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 254569.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.9672037760416666
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108971.0000 nanoseconds
sparsity Attention out: 0.9776204427083334
[Time] - Reshape Time-Space: 21186.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9781901041666666
sparsity Attention postReshape chunk - 1 output: 0.9697265625
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9827473958333334
[Time] - Transpose and LIF: 368572.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6422526041666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678955078125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6433919270833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967529296875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.967529296875
[Time] - Reshape: 254699.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.967529296875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108828.0000 nanoseconds
sparsity Attention out: 0.9735514322916666
[Time] - Reshape Time-Space: 20854.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.96875
sparsity Attention postReshape chunk - 1 output: 0.9654947916666666
sparsity Attention postReshape chunk - 2 output: 0.9837239583333334
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 367077.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9951171875
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6448567708333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676920572916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0486 seconds
Run 5 - Tempo totale del modello: 0.0486 secondi
[Time] - SPS.PSM: 0.0019 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.68603515625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97021484375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9712727864583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97021484375
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.9712727864583334
[Time] - Reshape: 257190.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.97021484375
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.9712727864583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 263191.0000 nanoseconds
sparsity Attention out: 0.97998046875
[Time] - Reshape Time-Space: 29812.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9886067708333334
sparsity Attention postReshape chunk - 1 output: 0.9860026041666666
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 368680.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6800130208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9716593424479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6702473958333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9713541666666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9713541666666666
sparsity Attention k_chunks: 0.969970703125
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 253961.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9713541666666666
sparsity Attention postReshape k_chunks: 0.969970703125
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 251807.0000 nanoseconds
sparsity Attention out: 0.9800618489583334
[Time] - Reshape Time-Space: 27285.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 381308.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6730143229166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688517252604166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6637369791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9663899739583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9663899739583334
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 262555.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9663899739583334
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107386.0000 nanoseconds
sparsity Attention out: 0.9759928385416666
[Time] - Reshape Time-Space: 21301.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9723307291666666
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9811197916666666
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 363264.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6572265625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683024088541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6586100260416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9666341145833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.9666341145833334
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 255257.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.9666341145833334
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256862.0000 nanoseconds
sparsity Attention out: 0.9784342447916666
[Time] - Reshape Time-Space: 28951.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9775390625
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9791666666666666
[Time] - Transpose and LIF: 373076.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6571451822916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9667561848958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6531575520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9664713541666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9666341145833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.9664713541666666
sparsity Attention v_chunks: 0.9666341145833334
[Time] - Reshape: 253386.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.9664713541666666
sparsity Attention postReshape v_chunks: 0.9666341145833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255664.0000 nanoseconds
sparsity Attention out: 0.9762369791666666
[Time] - Reshape Time-Space: 29668.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9794921875
sparsity Attention postReshape chunk - 1 output: 0.97265625
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9781901041666666
[Time] - Transpose and LIF: 370145.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9912109375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6482747395833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676513671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.646484375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 255161.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110947.0000 nanoseconds
sparsity Attention out: 0.974609375
[Time] - Reshape Time-Space: 20789.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9720052083333334
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 360530.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6409505208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96783447265625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6407877604166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 263641.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106600.0000 nanoseconds
sparsity Attention out: 0.9755859375
[Time] - Reshape Time-Space: 20465.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9664713541666666
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.9840494791666666
[Time] - Transpose and LIF: 362283.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6400553385416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670613606770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6388346354166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9659016927083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.9659016927083334
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 254870.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.9659016927083334
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 263902.0000 nanoseconds
sparsity Attention out: 0.975830078125
[Time] - Reshape Time-Space: 28733.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9817708333333334
sparsity Attention postReshape chunk - 1 output: 0.962890625
sparsity Attention postReshape chunk - 2 output: 0.9765625
sparsity Attention postReshape chunk - 3 output: 0.9820963541666666
[Time] - Transpose and LIF: 369023.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9886067708333334
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6385091145833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96636962890625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
[Time] - Spikeformer Time time: 0.0486 seconds
Run 6 - Tempo totale del modello: 0.0487 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6805826822916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9702962239583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9728190104166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.971923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9702962239583334
sparsity Attention k_chunks: 0.9728190104166666
sparsity Attention v_chunks: 0.971923828125
[Time] - Reshape: 253629.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9702962239583334
sparsity Attention postReshape k_chunks: 0.9728190104166666
sparsity Attention postReshape v_chunks: 0.971923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260236.0000 nanoseconds
sparsity Attention out: 0.9828287760416666
[Time] - Reshape Time-Space: 27734.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9928385416666666
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.9775390625
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 363512.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6846516927083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9692179361979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.675537109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9696451822916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9696451822916666
[Time] - Reshape: 252318.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9696451822916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109578.0000 nanoseconds
sparsity Attention out: 0.978271484375
[Time] - Reshape Time-Space: 20341.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9775390625
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 358170.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66552734375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9696248372395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.660400390625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966064453125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.966064453125
[Time] - Reshape: 253656.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.966064453125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 115618.0000 nanoseconds
sparsity Attention out: 0.9717610677083334
[Time] - Reshape Time-Space: 21242.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9759114583333334
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.96484375
[Time] - Transpose and LIF: 353836.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65478515625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9696451822916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6512044270833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9717610677083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.9717610677083334
[Time] - Reshape: 274437.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.9717610677083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 252223.0000 nanoseconds
sparsity Attention out: 0.9791666666666666
[Time] - Reshape Time-Space: 27146.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 359623.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9951171875
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6503092447916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96807861328125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.646728515625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9700520833333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9667154947916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9700520833333334
sparsity Attention k_chunks: 0.9667154947916666
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 256299.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9700520833333334
sparsity Attention postReshape k_chunks: 0.9667154947916666
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109503.0000 nanoseconds
sparsity Attention out: 0.9781087239583334
[Time] - Reshape Time-Space: 21857.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9892578125
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 378197.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6475423177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678141276041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.648193359375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 260549.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109387.0000 nanoseconds
sparsity Attention out: 0.9745279947916666
[Time] - Reshape Time-Space: 20551.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 369242.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.646240234375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9668782552083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6456705729166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9663899739583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.9663899739583334
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 253081.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.9663899739583334
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260382.0000 nanoseconds
sparsity Attention out: 0.9766438802083334
[Time] - Reshape Time-Space: 30086.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.9873046875
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 374892.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6424967447916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676513671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.640380859375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966796875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9667154947916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96533203125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966796875
sparsity Attention k_chunks: 0.9667154947916666
sparsity Attention v_chunks: 0.96533203125
[Time] - Reshape: 253219.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.966796875
sparsity Attention postReshape k_chunks: 0.9667154947916666
sparsity Attention postReshape v_chunks: 0.96533203125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259608.0000 nanoseconds
sparsity Attention out: 0.9718424479166666
[Time] - Reshape Time-Space: 29162.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9680989583333334
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9677734375
[Time] - Transpose and LIF: 380646.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9951171875
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.63623046875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670003255208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0492 seconds
Run 7 - Tempo totale del modello: 0.0493 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.68212890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9725748697916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9710286458333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97119140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9725748697916666
sparsity Attention k_chunks: 0.9710286458333334
sparsity Attention v_chunks: 0.97119140625
[Time] - Reshape: 253571.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9725748697916666
sparsity Attention postReshape k_chunks: 0.9710286458333334
sparsity Attention postReshape v_chunks: 0.97119140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 118523.0000 nanoseconds
sparsity Attention out: 0.98046875
[Time] - Reshape Time-Space: 22862.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 373973.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6954752604166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.97076416015625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6792805989583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9718424479166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9718424479166666
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 256649.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9718424479166666
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109008.0000 nanoseconds
sparsity Attention out: 0.97412109375
[Time] - Reshape Time-Space: 20499.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 382493.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6713053385416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701334635416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.663330078125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9702962239583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.9702962239583334
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 261350.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.9702962239583334
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260123.0000 nanoseconds
sparsity Attention out: 0.97900390625
[Time] - Reshape Time-Space: 28173.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9886067708333334
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 386260.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6591796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96905517578125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6554361979166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9701334635416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.9701334635416666
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 250846.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.9701334635416666
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255356.0000 nanoseconds
sparsity Attention out: 0.9765625
[Time] - Reshape Time-Space: 27183.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.9869791666666666
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.966796875
[Time] - Transpose and LIF: 359295.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6553548177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672444661458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6495768229166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9681803385416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676106770833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9681803385416666
sparsity Attention k_chunks: 0.9676106770833334
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 251641.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9681803385416666
sparsity Attention postReshape k_chunks: 0.9676106770833334
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107372.0000 nanoseconds
sparsity Attention out: 0.9720865885416666
[Time] - Reshape Time-Space: 28196.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.9716796875
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 363204.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6473795572916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96807861328125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.644775390625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9720052083333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9667154947916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9720052083333334
sparsity Attention v_chunks: 0.9667154947916666
[Time] - Reshape: 257570.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9720052083333334
sparsity Attention postReshape v_chunks: 0.9667154947916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258346.0000 nanoseconds
sparsity Attention out: 0.9778645833333334
[Time] - Reshape Time-Space: 28737.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.984375
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.9742838541666666
[Time] - Transpose and LIF: 369178.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.646728515625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.967041015625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6443684895833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9680989583333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970703125
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.9680989583333334
[Time] - Reshape: 252793.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970703125
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.9680989583333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 249022.0000 nanoseconds
sparsity Attention out: 0.9759928385416666
[Time] - Reshape Time-Space: 28471.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9736328125
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.982421875
[Time] - Transpose and LIF: 393669.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9951171875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6416829427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681193033854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.638916015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96630859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9672037760416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96630859375
sparsity Attention k_chunks: 0.9672037760416666
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 255420.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96630859375
sparsity Attention postReshape k_chunks: 0.9672037760416666
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109022.0000 nanoseconds
sparsity Attention out: 0.970947265625
[Time] - Reshape Time-Space: 21282.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9593098958333334
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 351935.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9918619791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.638916015625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9658610026041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0486 seconds
Run 8 - Tempo totale del modello: 0.0486 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.67333984375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9718424479166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9706217447916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.9718424479166666
sparsity Attention v_chunks: 0.9706217447916666
[Time] - Reshape: 253018.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.9718424479166666
sparsity Attention postReshape v_chunks: 0.9706217447916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 266689.0000 nanoseconds
sparsity Attention out: 0.982177734375
[Time] - Reshape Time-Space: 27571.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9869791666666666
sparsity Attention postReshape chunk - 2 output: 0.9781901041666666
sparsity Attention postReshape chunk - 3 output: 0.9814453125
[Time] - Transpose and LIF: 375056.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6728515625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9699300130208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6651204427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9715169270833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.97265625
sparsity Attention v_chunks: 0.9715169270833334
[Time] - Reshape: 253638.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.97265625
sparsity Attention postReshape v_chunks: 0.9715169270833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 248025.0000 nanoseconds
sparsity Attention out: 0.9825846354166666
[Time] - Reshape Time-Space: 27761.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9775390625
sparsity Attention postReshape chunk - 3 output: 0.9788411458333334
[Time] - Transpose and LIF: 374318.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6687825520833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681396484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.665283203125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9662272135416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.9662272135416666
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 252413.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.9662272135416666
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106530.0000 nanoseconds
sparsity Attention out: 0.968994140625
[Time] - Reshape Time-Space: 21167.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9772135416666666
sparsity Attention postReshape chunk - 1 output: 0.9671223958333334
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.958984375
[Time] - Transpose and LIF: 380094.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9947916666666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6682942708333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.968505859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6634114583333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9708658854166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966796875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9708658854166666
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.966796875
[Time] - Reshape: 253330.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9708658854166666
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.966796875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260688.0000 nanoseconds
sparsity Attention out: 0.9761555989583334
[Time] - Reshape Time-Space: 27867.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.9694010416666666
sparsity Attention postReshape chunk - 2 output: 0.9817708333333334
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 372348.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6505533854166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96795654296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6505533854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9657389322916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.9657389322916666
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 254789.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.9657389322916666
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256433.0000 nanoseconds
sparsity Attention out: 0.9757486979166666
[Time] - Reshape Time-Space: 28578.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9742838541666666
sparsity Attention postReshape chunk - 1 output: 0.9723307291666666
sparsity Attention postReshape chunk - 2 output: 0.9817708333333334
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 374332.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.994140625
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6536458333333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96746826171875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.64990234375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9652506510416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.965087890625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9652506510416666
sparsity Attention k_chunks: 0.965087890625
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 252590.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9652506510416666
sparsity Attention postReshape k_chunks: 0.965087890625
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106702.0000 nanoseconds
sparsity Attention out: 0.9683430989583334
[Time] - Reshape Time-Space: 20845.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9671223958333334
sparsity Attention postReshape chunk - 1 output: 0.9684244791666666
sparsity Attention postReshape chunk - 2 output: 0.9661458333333334
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 367653.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6438802083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675496419270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6454264322916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96630859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9657389322916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96630859375
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.9657389322916666
[Time] - Reshape: 260906.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96630859375
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.9657389322916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107385.0000 nanoseconds
sparsity Attention out: 0.9747721354166666
[Time] - Reshape Time-Space: 20643.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9710286458333334
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 379222.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6434733072916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.966064453125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6427408854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96533203125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9659016927083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.964111328125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96533203125
sparsity Attention k_chunks: 0.9659016927083334
sparsity Attention v_chunks: 0.964111328125
[Time] - Reshape: 255663.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96533203125
sparsity Attention postReshape k_chunks: 0.9659016927083334
sparsity Attention postReshape v_chunks: 0.964111328125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 250047.0000 nanoseconds
sparsity Attention out: 0.9681803385416666
[Time] - Reshape Time-Space: 29062.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9654947916666666
sparsity Attention postReshape chunk - 1 output: 0.9606119791666666
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 381354.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9954427083333334
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6387532552083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676920572916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
[Time] - Spikeformer Time time: 0.0488 seconds
Run 9 - Tempo totale del modello: 0.0488 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6776529947916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9725748697916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9711100260416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9715169270833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9725748697916666
sparsity Attention k_chunks: 0.9711100260416666
sparsity Attention v_chunks: 0.9715169270833334
[Time] - Reshape: 251876.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9725748697916666
sparsity Attention postReshape k_chunks: 0.9711100260416666
sparsity Attention postReshape v_chunks: 0.9715169270833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 254825.0000 nanoseconds
sparsity Attention out: 0.9803059895833334
[Time] - Reshape Time-Space: 29325.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9811197916666666
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9781901041666666
[Time] - Transpose and LIF: 382077.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6841634114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.97039794921875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9713541666666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970947265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9671223958333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9713541666666666
sparsity Attention k_chunks: 0.970947265625
sparsity Attention v_chunks: 0.9671223958333334
[Time] - Reshape: 255000.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9713541666666666
sparsity Attention postReshape k_chunks: 0.970947265625
sparsity Attention postReshape v_chunks: 0.9671223958333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109400.0000 nanoseconds
sparsity Attention out: 0.9763997395833334
[Time] - Reshape Time-Space: 20601.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.9899088541666666
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9635416666666666
[Time] - Transpose and LIF: 371203.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6759440104166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701334635416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6651204427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9693196614583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9693196614583334
[Time] - Reshape: 256474.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9693196614583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 263001.0000 nanoseconds
sparsity Attention out: 0.978759765625
[Time] - Reshape Time-Space: 28181.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9850260416666666
sparsity Attention postReshape chunk - 3 output: 0.9651692708333334
[Time] - Transpose and LIF: 372545.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6649576822916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685872395833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6624348958333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.971435546875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9691569010416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.971435546875
sparsity Attention k_chunks: 0.9691569010416666
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 250523.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.971435546875
sparsity Attention postReshape k_chunks: 0.9691569010416666
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 246303.0000 nanoseconds
sparsity Attention out: 0.9778645833333334
[Time] - Reshape Time-Space: 27600.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.9860026041666666
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.9752604166666666
[Time] - Transpose and LIF: 366675.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6611328125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688720703125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6577962239583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96630859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966796875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.96630859375
sparsity Attention v_chunks: 0.966796875
[Time] - Reshape: 263889.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.96630859375
sparsity Attention postReshape v_chunks: 0.966796875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108341.0000 nanoseconds
sparsity Attention out: 0.971923828125
[Time] - Reshape Time-Space: 23144.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.978515625
sparsity Attention postReshape chunk - 1 output: 0.9677734375
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9654947916666666
[Time] - Transpose and LIF: 454098.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6568196614583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96722412109375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6527506510416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9706217447916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9697265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.9706217447916666
sparsity Attention v_chunks: 0.9697265625
[Time] - Reshape: 252161.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.9706217447916666
sparsity Attention postReshape v_chunks: 0.9697265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106983.0000 nanoseconds
sparsity Attention out: 0.979736328125
[Time] - Reshape Time-Space: 22178.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 375002.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6529947916666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671223958333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6487630208333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96728515625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9652506510416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.96728515625
sparsity Attention v_chunks: 0.9652506510416666
[Time] - Reshape: 253719.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.96728515625
sparsity Attention postReshape v_chunks: 0.9652506510416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 263325.0000 nanoseconds
sparsity Attention out: 0.972412109375
[Time] - Reshape Time-Space: 30042.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98828125
sparsity Attention postReshape chunk - 1 output: 0.9671223958333334
sparsity Attention postReshape chunk - 2 output: 0.9664713541666666
sparsity Attention postReshape chunk - 3 output: 0.9677734375
[Time] - Transpose and LIF: 375244.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64208984375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96722412109375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6387532552083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 277456.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 245852.0000 nanoseconds
sparsity Attention out: 0.9751790364583334
[Time] - Reshape Time-Space: 25965.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9716796875
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9791666666666666
[Time] - Transpose and LIF: 366418.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9934895833333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6382649739583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670206705729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0488 seconds
Run 10 - Tempo totale del modello: 0.0488 secondi
[Time] - SPS.PSM: 0.0019 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.680419921875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970458984375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9715169270833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9703776041666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970458984375
sparsity Attention k_chunks: 0.9715169270833334
sparsity Attention v_chunks: 0.9703776041666666
[Time] - Reshape: 255324.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970458984375
sparsity Attention postReshape k_chunks: 0.9715169270833334
sparsity Attention postReshape v_chunks: 0.9703776041666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109087.0000 nanoseconds
sparsity Attention out: 0.9791666666666666
[Time] - Reshape Time-Space: 21796.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98828125
sparsity Attention postReshape chunk - 1 output: 0.984375
sparsity Attention postReshape chunk - 2 output: 0.9671223958333334
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 361254.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6874186197916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9697062174479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.67431640625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.969970703125
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 263825.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.969970703125
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 254769.0000 nanoseconds
sparsity Attention out: 0.9772135416666666
[Time] - Reshape Time-Space: 27822.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.9847005208333334
sparsity Attention postReshape chunk - 2 output: 0.9658203125
sparsity Attention postReshape chunk - 3 output: 0.9778645833333334
[Time] - Transpose and LIF: 386734.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6741536458333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690755208333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6700846354166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 252743.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 252454.0000 nanoseconds
sparsity Attention out: 0.9779459635416666
[Time] - Reshape Time-Space: 28139.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9817708333333334
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9772135416666666
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 387789.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6597493489583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687093098958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6600748697916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 251068.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 116489.0000 nanoseconds
sparsity Attention out: 0.9775390625
[Time] - Reshape Time-Space: 21649.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9772135416666666
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 354823.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6536458333333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96826171875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.65234375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9696451822916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.9696451822916666
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 257155.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.9696451822916666
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105833.0000 nanoseconds
sparsity Attention out: 0.9790852864583334
[Time] - Reshape Time-Space: 21050.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9879557291666666
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9781901041666666
[Time] - Transpose and LIF: 366425.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6520182291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9668375651041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6505533854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9677734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676106770833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9677734375
sparsity Attention k_chunks: 0.9676106770833334
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 255814.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9677734375
sparsity Attention postReshape k_chunks: 0.9676106770833334
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 268041.0000 nanoseconds
sparsity Attention out: 0.9747721354166666
[Time] - Reshape Time-Space: 28756.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9749348958333334
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 383865.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.99609375
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0045 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6494954427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671630859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6466471354166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 256519.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 250265.0000 nanoseconds
sparsity Attention out: 0.97412109375
[Time] - Reshape Time-Space: 27422.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9736328125
sparsity Attention postReshape chunk - 1 output: 0.9713541666666666
sparsity Attention postReshape chunk - 2 output: 0.9814453125
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 383966.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.99609375
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6478678385416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9667154947916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6456705729166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9644368489583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9661458333333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.9644368489583334
sparsity Attention v_chunks: 0.9661458333333334
[Time] - Reshape: 261299.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.9644368489583334
sparsity Attention postReshape v_chunks: 0.9661458333333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107589.0000 nanoseconds
sparsity Attention out: 0.9685872395833334
[Time] - Reshape Time-Space: 21180.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9615885416666666
sparsity Attention postReshape chunk - 1 output: 0.9697265625
sparsity Attention postReshape chunk - 2 output: 0.9703776041666666
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 374236.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6409505208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671834309895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0491 seconds
Run 11 - Tempo totale del modello: 0.0491 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0005 seconds
sparsity x_for_qkv: 0.6815592447916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.971923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9720052083333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.971923828125
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.9720052083333334
[Time] - Reshape: 264521.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.971923828125
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.9720052083333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109465.0000 nanoseconds
sparsity Attention out: 0.9803059895833334
[Time] - Reshape Time-Space: 22669.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9772135416666666
sparsity Attention postReshape chunk - 3 output: 0.978515625
[Time] - Transpose and LIF: 378178.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.67919921875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9711507161458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6680501302083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9705403645833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9717610677083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.9705403645833334
sparsity Attention v_chunks: 0.9717610677083334
[Time] - Reshape: 254243.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.9705403645833334
sparsity Attention postReshape v_chunks: 0.9717610677083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258480.0000 nanoseconds
sparsity Attention out: 0.9801432291666666
[Time] - Reshape Time-Space: 29009.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9830729166666666
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 381038.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6689453125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9697469075520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.662109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698893229166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9688313802083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9698893229166666
sparsity Attention v_chunks: 0.9688313802083334
[Time] - Reshape: 252527.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9698893229166666
sparsity Attention postReshape v_chunks: 0.9688313802083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107125.0000 nanoseconds
sparsity Attention out: 0.9779459635416666
[Time] - Reshape Time-Space: 21797.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.9723307291666666
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9833984375
[Time] - Transpose and LIF: 359235.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66162109375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679361979166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6559244791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 254254.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107663.0000 nanoseconds
sparsity Attention out: 0.9740397135416666
[Time] - Reshape Time-Space: 21130.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9755859375
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9671223958333334
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 358900.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6544596354166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96795654296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6486002604166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9659830729166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966796875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9659830729166666
sparsity Attention v_chunks: 0.966796875
[Time] - Reshape: 260799.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9659830729166666
sparsity Attention postReshape v_chunks: 0.966796875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255782.0000 nanoseconds
sparsity Attention out: 0.9727376302083334
[Time] - Reshape Time-Space: 29496.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9765625
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 376401.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.645263671875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672648111979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6416829427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9669596354166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9669596354166666
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 253386.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9669596354166666
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 261736.0000 nanoseconds
sparsity Attention out: 0.97998046875
[Time] - Reshape Time-Space: 29691.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.974609375
sparsity Attention postReshape chunk - 2 output: 0.9873046875
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 384497.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64111328125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679361979166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6415201822916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9646809895833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9667154947916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9646809895833334
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9667154947916666
[Time] - Reshape: 251998.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9646809895833334
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9667154947916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 116302.0000 nanoseconds
sparsity Attention out: 0.970947265625
[Time] - Reshape Time-Space: 22702.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.97265625
sparsity Attention postReshape chunk - 1 output: 0.955078125
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9798177083333334
[Time] - Transpose and LIF: 365654.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.63916015625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96783447265625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.64111328125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9664713541666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9638671875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9664713541666666
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.9638671875
[Time] - Reshape: 254182.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9664713541666666
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.9638671875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255724.0000 nanoseconds
sparsity Attention out: 0.974365234375
[Time] - Reshape Time-Space: 27598.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9739583333333334
sparsity Attention postReshape chunk - 1 output: 0.97265625
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 366276.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.640380859375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9668782552083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
[Time] - Spikeformer Time time: 0.0488 seconds
Run 12 - Tempo totale del modello: 0.0488 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6813151041666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.97265625
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 257525.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.97265625
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255795.0000 nanoseconds
sparsity Attention out: 0.9810384114583334
[Time] - Reshape Time-Space: 28508.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9847005208333334
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9837239583333334
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 464503.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6769205729166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9705607096354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6659342447916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 261100.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107579.0000 nanoseconds
sparsity Attention out: 0.9814453125
[Time] - Reshape Time-Space: 20847.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9869791666666666
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9772135416666666
[Time] - Transpose and LIF: 363284.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6637369791666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678751627604166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6586100260416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968994140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968994140625
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 258654.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968994140625
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109785.0000 nanoseconds
sparsity Attention out: 0.9775390625
[Time] - Reshape Time-Space: 21674.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9710286458333334
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.9781901041666666
sparsity Attention postReshape chunk - 3 output: 0.9853515625
[Time] - Transpose and LIF: 366366.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6539713541666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674479166666666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6490885416666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9664713541666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9651692708333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9664713541666666
sparsity Attention k_chunks: 0.9651692708333334
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 252950.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9664713541666666
sparsity Attention postReshape k_chunks: 0.9651692708333334
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259564.0000 nanoseconds
sparsity Attention out: 0.9732259114583334
[Time] - Reshape Time-Space: 35855.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9755859375
sparsity Attention postReshape chunk - 1 output: 0.9752604166666666
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 381853.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64599609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675089518229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.646240234375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9639485677083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9647623697916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9666341145833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9639485677083334
sparsity Attention k_chunks: 0.9647623697916666
sparsity Attention v_chunks: 0.9666341145833334
[Time] - Reshape: 264974.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9639485677083334
sparsity Attention postReshape k_chunks: 0.9647623697916666
sparsity Attention postReshape v_chunks: 0.9666341145833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 250306.0000 nanoseconds
sparsity Attention out: 0.969970703125
[Time] - Reshape Time-Space: 28411.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.96875
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9602864583333334
[Time] - Transpose and LIF: 369480.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9947916666666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64501953125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673665364583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6485188802083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9663899739583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96630859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9663899739583334
sparsity Attention k_chunks: 0.96630859375
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 264421.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9663899739583334
sparsity Attention postReshape k_chunks: 0.96630859375
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107264.0000 nanoseconds
sparsity Attention out: 0.9755045572916666
[Time] - Reshape Time-Space: 21695.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9775390625
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 371785.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.645751953125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677530924479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.642822265625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9668782552083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9644368489583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9649251302083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9668782552083334
sparsity Attention k_chunks: 0.9644368489583334
sparsity Attention v_chunks: 0.9649251302083334
[Time] - Reshape: 247871.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9668782552083334
sparsity Attention postReshape k_chunks: 0.9644368489583334
sparsity Attention postReshape v_chunks: 0.9649251302083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 266429.0000 nanoseconds
sparsity Attention out: 0.9677734375
[Time] - Reshape Time-Space: 31018.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9664713541666666
sparsity Attention postReshape chunk - 1 output: 0.9599609375
sparsity Attention postReshape chunk - 2 output: 0.9794921875
sparsity Attention postReshape chunk - 3 output: 0.9651692708333334
[Time] - Transpose and LIF: 459230.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6442057291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669392903645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6419270833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9664713541666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9651692708333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9652506510416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9664713541666666
sparsity Attention k_chunks: 0.9651692708333334
sparsity Attention v_chunks: 0.9652506510416666
[Time] - Reshape: 274405.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9664713541666666
sparsity Attention postReshape k_chunks: 0.9651692708333334
sparsity Attention postReshape v_chunks: 0.9652506510416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 3291166.0000 nanoseconds
sparsity Attention out: 0.9659016927083334
[Time] - Reshape Time-Space: 37705.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.955078125
sparsity Attention postReshape chunk - 1 output: 0.9700520833333334
sparsity Attention postReshape chunk - 2 output: 0.9690755208333334
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 389163.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0071 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6378580729166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673665364583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0519 seconds
Run 13 - Tempo totale del modello: 0.0519 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6790364583333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9708658854166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9708658854166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970458984375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9708658854166666
sparsity Attention k_chunks: 0.9708658854166666
sparsity Attention v_chunks: 0.970458984375
[Time] - Reshape: 268517.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9708658854166666
sparsity Attention postReshape k_chunks: 0.9708658854166666
sparsity Attention postReshape v_chunks: 0.970458984375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 266084.0000 nanoseconds
sparsity Attention out: 0.9823404947916666
[Time] - Reshape Time-Space: 28134.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9928385416666666
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.9759114583333334
[Time] - Transpose and LIF: 384590.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6813151041666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9718017578125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6693522135416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 257726.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110114.0000 nanoseconds
sparsity Attention out: 0.9761555989583334
[Time] - Reshape Time-Space: 23314.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9817708333333334
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 360771.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.666015625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9696858723958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6634928385416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9710286458333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.9710286458333334
[Time] - Reshape: 256740.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.9710286458333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255886.0000 nanoseconds
sparsity Attention out: 0.97607421875
[Time] - Reshape Time-Space: 29303.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9798177083333334
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 383015.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6572265625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690144856770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.65478515625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9712727864583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9712727864583334
sparsity Attention k_chunks: 0.970703125
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 262051.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9712727864583334
sparsity Attention postReshape k_chunks: 0.970703125
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 251810.0000 nanoseconds
sparsity Attention out: 0.9798177083333334
[Time] - Reshape Time-Space: 27641.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9755859375
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.984375
[Time] - Transpose and LIF: 391624.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6527506510416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676106770833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6470540364583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9705403645833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.9705403645833334
[Time] - Reshape: 254947.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.9705403645833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108879.0000 nanoseconds
sparsity Attention out: 0.9759928385416666
[Time] - Reshape Time-Space: 20746.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 360122.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6475423177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681193033854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6416829427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9662272135416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.9662272135416666
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 253484.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.9662272135416666
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 261806.0000 nanoseconds
sparsity Attention out: 0.9761555989583334
[Time] - Reshape Time-Space: 27472.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.97265625
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9775390625
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 375850.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9931640625
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6407063802083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665120442708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6412760416666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966552734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.966552734375
[Time] - Reshape: 267458.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.966552734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258302.0000 nanoseconds
sparsity Attention out: 0.971923828125
[Time] - Reshape Time-Space: 27832.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9736328125
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.9654947916666666
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 374169.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.634033203125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9666341145833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6341145833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 267474.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109746.0000 nanoseconds
sparsity Attention out: 0.9755859375
[Time] - Reshape Time-Space: 21269.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9716796875
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 441624.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6351725260416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96636962890625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0490 seconds
Run 14 - Tempo totale del modello: 0.0490 secondi
[Time] - SPS.PSM: 0.0019 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6787923177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970458984375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.970458984375
sparsity Attention v_chunks: 0.97265625
[Time] - Reshape: 260270.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.970458984375
sparsity Attention postReshape v_chunks: 0.97265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108086.0000 nanoseconds
sparsity Attention out: 0.9808756510416666
[Time] - Reshape Time-Space: 20402.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9869791666666666
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.984375
[Time] - Transpose and LIF: 381106.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6678873697916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9707234700520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6612141927083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9720865885416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9711100260416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9710286458333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9720865885416666
sparsity Attention k_chunks: 0.9711100260416666
sparsity Attention v_chunks: 0.9710286458333334
[Time] - Reshape: 255337.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9720865885416666
sparsity Attention postReshape k_chunks: 0.9711100260416666
sparsity Attention postReshape v_chunks: 0.9710286458333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 257969.0000 nanoseconds
sparsity Attention out: 0.98193359375
[Time] - Reshape Time-Space: 28169.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9827473958333334
[Time] - Transpose and LIF: 394370.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6575520833333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686686197916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6537272135416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698893229166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9698893229166666
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 254432.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9698893229166666
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 265670.0000 nanoseconds
sparsity Attention out: 0.974365234375
[Time] - Reshape Time-Space: 29080.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9612630208333334
sparsity Attention postReshape chunk - 3 output: 0.9674479166666666
[Time] - Transpose and LIF: 385376.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6570638020833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687093098958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6532389322916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9674479166666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9678548177083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9674479166666666
sparsity Attention k_chunks: 0.9678548177083334
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 258812.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9674479166666666
sparsity Attention postReshape k_chunks: 0.9678548177083334
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107494.0000 nanoseconds
sparsity Attention out: 0.97607421875
[Time] - Reshape Time-Space: 20898.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9645182291666666
[Time] - Transpose and LIF: 367943.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6494954427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9691365559895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.648193359375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9683430989583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.9683430989583334
[Time] - Reshape: 265611.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.9683430989583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258422.0000 nanoseconds
sparsity Attention out: 0.9718424479166666
[Time] - Reshape Time-Space: 29136.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9664713541666666
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9680989583333334
sparsity Attention postReshape chunk - 3 output: 0.9733072916666666
[Time] - Transpose and LIF: 393624.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6439615885416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683430989583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6416829427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9661458333333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9661458333333334
[Time] - Reshape: 252714.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9661458333333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 252723.0000 nanoseconds
sparsity Attention out: 0.973388671875
[Time] - Reshape Time-Space: 28079.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.9765625
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 392565.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9951171875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6402180989583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675699869791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6422526041666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966064453125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966064453125
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 402837.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.966064453125
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 122275.0000 nanoseconds
sparsity Attention out: 0.9751790364583334
[Time] - Reshape Time-Space: 23534.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9781901041666666
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 378293.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6407877604166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678141276041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6416829427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9661458333333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9657389322916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96630859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9661458333333334
sparsity Attention k_chunks: 0.9657389322916666
sparsity Attention v_chunks: 0.96630859375
[Time] - Reshape: 253416.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9661458333333334
sparsity Attention postReshape k_chunks: 0.9657389322916666
sparsity Attention postReshape v_chunks: 0.96630859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108327.0000 nanoseconds
sparsity Attention out: 0.9674479166666666
[Time] - Reshape Time-Space: 21405.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9593098958333334
sparsity Attention postReshape chunk - 1 output: 0.974609375
sparsity Attention postReshape chunk - 2 output: 0.96875
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 363457.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9944661458333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6398111979166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96600341796875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0487 seconds
Run 15 - Tempo totale del modello: 0.0487 secondi
[Time] - SPS.PSM: 0.0022 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.68212890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970947265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.970703125
sparsity Attention v_chunks: 0.970947265625
[Time] - Reshape: 256783.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.970703125
sparsity Attention postReshape v_chunks: 0.970947265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258278.0000 nanoseconds
sparsity Attention out: 0.9807942708333334
[Time] - Reshape Time-Space: 28383.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9908854166666666
sparsity Attention postReshape chunk - 1 output: 0.9847005208333334
sparsity Attention postReshape chunk - 2 output: 0.9690755208333334
sparsity Attention postReshape chunk - 3 output: 0.978515625
[Time] - Transpose and LIF: 378462.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6819661458333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9699910481770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.671142578125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970458984375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.970458984375
[Time] - Reshape: 252910.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.970458984375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 280275.0000 nanoseconds
sparsity Attention out: 0.9745279947916666
[Time] - Reshape Time-Space: 30960.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9759114583333334
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 389132.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6702473958333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9691569010416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.667724609375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970458984375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970458984375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970458984375
sparsity Attention k_chunks: 0.970458984375
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 255510.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970458984375
sparsity Attention postReshape k_chunks: 0.970458984375
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107129.0000 nanoseconds
sparsity Attention out: 0.9781901041666666
[Time] - Reshape Time-Space: 21613.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.9892578125
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 358616.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66357421875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9691569010416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6576334635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 263172.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 270950.0000 nanoseconds
sparsity Attention out: 0.9781087239583334
[Time] - Reshape Time-Space: 31688.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 384616.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9951171875
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6524251302083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680989583333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6494954427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9649251302083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96826171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9667154947916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9649251302083334
sparsity Attention k_chunks: 0.96826171875
sparsity Attention v_chunks: 0.9667154947916666
[Time] - Reshape: 254466.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9649251302083334
sparsity Attention postReshape k_chunks: 0.96826171875
sparsity Attention postReshape v_chunks: 0.9667154947916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253287.0000 nanoseconds
sparsity Attention out: 0.9696451822916666
[Time] - Reshape Time-Space: 28935.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9645182291666666
sparsity Attention postReshape chunk - 1 output: 0.970703125
sparsity Attention postReshape chunk - 2 output: 0.9674479166666666
sparsity Attention postReshape chunk - 3 output: 0.9759114583333334
[Time] - Transpose and LIF: 376739.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6509602864583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672648111979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6499837239583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9661458333333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9651692708333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.9661458333333334
sparsity Attention v_chunks: 0.9651692708333334
[Time] - Reshape: 255163.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.9661458333333334
sparsity Attention postReshape v_chunks: 0.9651692708333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107559.0000 nanoseconds
sparsity Attention out: 0.9715983072916666
[Time] - Reshape Time-Space: 21457.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9742838541666666
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9599609375
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 359305.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6458333333333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9666544596354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.64453125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9672037760416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9649251302083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.9672037760416666
sparsity Attention v_chunks: 0.9649251302083334
[Time] - Reshape: 278791.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.9672037760416666
sparsity Attention postReshape v_chunks: 0.9649251302083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105704.0000 nanoseconds
sparsity Attention out: 0.9740397135416666
[Time] - Reshape Time-Space: 19930.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9674479166666666
[Time] - Transpose and LIF: 348158.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6438802083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9666341145833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6385904947916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9677734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9691569010416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9654947916666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9677734375
sparsity Attention k_chunks: 0.9691569010416666
sparsity Attention v_chunks: 0.9654947916666666
[Time] - Reshape: 283805.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9677734375
sparsity Attention postReshape k_chunks: 0.9691569010416666
sparsity Attention postReshape v_chunks: 0.9654947916666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 245800.0000 nanoseconds
sparsity Attention out: 0.9774576822916666
[Time] - Reshape Time-Space: 26471.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9817708333333334
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9814453125
[Time] - Transpose and LIF: 375547.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6402180989583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9664103190104166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0490 seconds
Run 16 - Tempo totale del modello: 0.0491 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6768391927083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9713541666666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9710286458333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.9713541666666666
sparsity Attention v_chunks: 0.9710286458333334
[Time] - Reshape: 267334.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.9713541666666666
sparsity Attention postReshape v_chunks: 0.9710286458333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 250091.0000 nanoseconds
sparsity Attention out: 0.9785970052083334
[Time] - Reshape Time-Space: 28712.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9869791666666666
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 385369.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6682942708333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9713541666666666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6597493489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9707845052083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967529296875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.9707845052083334
sparsity Attention v_chunks: 0.967529296875
[Time] - Reshape: 253249.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.9707845052083334
sparsity Attention postReshape v_chunks: 0.967529296875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108464.0000 nanoseconds
sparsity Attention out: 0.9771321614583334
[Time] - Reshape Time-Space: 21498.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.984375
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9615885416666666
sparsity Attention postReshape chunk - 3 output: 0.9798177083333334
[Time] - Transpose and LIF: 369824.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.657958984375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688313802083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.650390625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 252339.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256138.0000 nanoseconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 27622.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 362669.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6521809895833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683634440104166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6551106770833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9695638020833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9701334635416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96435546875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9695638020833334
sparsity Attention k_chunks: 0.9701334635416666
sparsity Attention v_chunks: 0.96435546875
[Time] - Reshape: 257583.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9695638020833334
sparsity Attention postReshape k_chunks: 0.9701334635416666
sparsity Attention postReshape v_chunks: 0.96435546875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 247967.0000 nanoseconds
sparsity Attention out: 0.9796549479166666
[Time] - Reshape Time-Space: 26974.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.984375
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 451158.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65087890625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9684041341145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6483561197916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9695638020833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9695638020833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969482421875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9695638020833334
sparsity Attention k_chunks: 0.9695638020833334
sparsity Attention v_chunks: 0.969482421875
[Time] - Reshape: 260333.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9695638020833334
sparsity Attention postReshape k_chunks: 0.9695638020833334
sparsity Attention postReshape v_chunks: 0.969482421875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107670.0000 nanoseconds
sparsity Attention out: 0.9794921875
[Time] - Reshape Time-Space: 21401.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.9778645833333334
sparsity Attention postReshape chunk - 2 output: 0.9791666666666666
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 372975.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6465657552083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690958658854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6433919270833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 251998.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264205.0000 nanoseconds
sparsity Attention out: 0.9796549479166666
[Time] - Reshape Time-Space: 28935.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.97265625
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9951171875
[Time] - Transpose and LIF: 382943.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.646484375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680582682291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6453450520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968994140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9696451822916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968994140625
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9696451822916666
[Time] - Reshape: 252448.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968994140625
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9696451822916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 249738.0000 nanoseconds
sparsity Attention out: 0.98046875
[Time] - Reshape Time-Space: 28268.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9755859375
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.98046875
sparsity Attention postReshape chunk - 3 output: 0.9876302083333334
[Time] - Transpose and LIF: 368893.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6429036458333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678955078125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6448567708333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9678548177083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.9678548177083334
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 252635.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.9678548177083334
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106345.0000 nanoseconds
sparsity Attention out: 0.9742838541666666
[Time] - Reshape Time-Space: 20278.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9723307291666666
sparsity Attention postReshape chunk - 1 output: 0.9759114583333334
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 373839.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6435546875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665120442708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0485 seconds
Run 17 - Tempo totale del modello: 0.0485 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.680908203125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97021484375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9720865885416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97021484375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97021484375
sparsity Attention k_chunks: 0.9720865885416666
sparsity Attention v_chunks: 0.97021484375
[Time] - Reshape: 270546.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.97021484375
sparsity Attention postReshape k_chunks: 0.9720865885416666
sparsity Attention postReshape v_chunks: 0.97021484375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108240.0000 nanoseconds
sparsity Attention out: 0.981201171875
[Time] - Reshape Time-Space: 21274.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 372694.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6904296875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96929931640625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6805826822916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9712727864583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.970703125
sparsity Attention v_chunks: 0.9712727864583334
[Time] - Reshape: 252704.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.970703125
sparsity Attention postReshape v_chunks: 0.9712727864583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258604.0000 nanoseconds
sparsity Attention out: 0.9806315104166666
[Time] - Reshape Time-Space: 28944.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.986328125
[Time] - Transpose and LIF: 382775.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6770833333333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690755208333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6647135416666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96630859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.96630859375
[Time] - Reshape: 254519.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.96630859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 251677.0000 nanoseconds
sparsity Attention out: 0.98095703125
[Time] - Reshape Time-Space: 28347.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.990234375
sparsity Attention postReshape chunk - 1 output: 0.986328125
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 374880.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6676432291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96844482421875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6604817708333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9661458333333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9661458333333334
[Time] - Reshape: 254260.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9661458333333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107988.0000 nanoseconds
sparsity Attention out: 0.974609375
[Time] - Reshape Time-Space: 20466.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9736328125
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9765625
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 374750.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.99609375
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9954427083333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6537272135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9699503580729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6502278645833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9680989583333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9680989583333334
[Time] - Reshape: 260031.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9680989583333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 254988.0000 nanoseconds
sparsity Attention out: 0.975830078125
[Time] - Reshape Time-Space: 28063.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.9752604166666666
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9791666666666666
[Time] - Transpose and LIF: 372224.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6488444010416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686075846354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6494140625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9662272135416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9662272135416666
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 262736.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9662272135416666
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260059.0000 nanoseconds
sparsity Attention out: 0.97314453125
[Time] - Reshape Time-Space: 31068.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9716796875
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9775390625
sparsity Attention postReshape chunk - 3 output: 0.96484375
[Time] - Transpose and LIF: 358682.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.643798828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675496419270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6451822916666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9666341145833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9666341145833334
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 257781.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9666341145833334
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110047.0000 nanoseconds
sparsity Attention out: 0.9756673177083334
[Time] - Reshape Time-Space: 23116.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9739583333333334
sparsity Attention postReshape chunk - 1 output: 0.9736328125
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9794921875
[Time] - Transpose and LIF: 344024.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.640380859375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677327473958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6392415364583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967529296875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.967529296875
[Time] - Reshape: 356630.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.967529296875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 111193.0000 nanoseconds
sparsity Attention out: 0.9720865885416666
[Time] - Reshape Time-Space: 22467.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.970703125
sparsity Attention postReshape chunk - 2 output: 0.9680989583333334
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 357146.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.63916015625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9668782552083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0505 seconds
Run 18 - Tempo totale del modello: 0.0505 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6744791666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9705403645833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9736328125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698893229166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9705403645833334
sparsity Attention k_chunks: 0.9736328125
sparsity Attention v_chunks: 0.9698893229166666
[Time] - Reshape: 267271.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9705403645833334
sparsity Attention postReshape k_chunks: 0.9736328125
sparsity Attention postReshape v_chunks: 0.9698893229166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110321.0000 nanoseconds
sparsity Attention out: 0.9822591145833334
[Time] - Reshape Time-Space: 22235.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9830729166666666
[Time] - Transpose and LIF: 380907.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6713053385416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.97113037109375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6602376302083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970458984375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970458984375
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 302275.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970458984375
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259030.0000 nanoseconds
sparsity Attention out: 0.9775390625
[Time] - Reshape Time-Space: 28903.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.974609375
sparsity Attention postReshape chunk - 2 output: 0.9817708333333334
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 380476.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.660400390625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9702962239583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.65771484375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9710286458333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.9710286458333334
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 252083.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.9710286458333334
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110564.0000 nanoseconds
sparsity Attention out: 0.98095703125
[Time] - Reshape Time-Space: 22536.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9811197916666666
sparsity Attention postReshape chunk - 1 output: 0.96875
sparsity Attention postReshape chunk - 2 output: 0.9840494791666666
sparsity Attention postReshape chunk - 3 output: 0.9899088541666666
[Time] - Transpose and LIF: 364284.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6585286458333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687703450520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6505533854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9683430989583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.9683430989583334
[Time] - Reshape: 256308.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.9683430989583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 111317.0000 nanoseconds
sparsity Attention out: 0.9755045572916666
[Time] - Reshape Time-Space: 23437.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9713541666666666
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 369006.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.651611328125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685465494791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6487630208333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9691569010416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9691569010416666
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 313064.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9691569010416666
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255022.0000 nanoseconds
sparsity Attention out: 0.9757486979166666
[Time] - Reshape Time-Space: 29292.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9778645833333334
[Time] - Transpose and LIF: 389044.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64208984375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685465494791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.639404296875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 255902.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 249589.0000 nanoseconds
sparsity Attention out: 0.9815266927083334
[Time] - Reshape Time-Space: 37836.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9873046875
sparsity Attention postReshape chunk - 1 output: 0.9856770833333334
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9788411458333334
[Time] - Transpose and LIF: 365200.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6411946614583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671834309895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.638916015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9678548177083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.9678548177083334
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 260825.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.9678548177083334
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 116975.0000 nanoseconds
sparsity Attention out: 0.9756673177083334
[Time] - Reshape Time-Space: 21847.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9720052083333334
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 363524.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6397298177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672444661458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6393229166666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9668782552083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676106770833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9668782552083334
sparsity Attention k_chunks: 0.9676106770833334
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 279642.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9668782552083334
sparsity Attention postReshape k_chunks: 0.9676106770833334
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 254899.0000 nanoseconds
sparsity Attention out: 0.9750162760416666
[Time] - Reshape Time-Space: 29564.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.9697265625
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9856770833333334
[Time] - Transpose and LIF: 383783.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9957682291666666
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.637939453125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665730794270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0526 seconds
Run 19 - Tempo totale del modello: 0.0526 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6809895833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970947265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9703776041666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970947265625
sparsity Attention k_chunks: 0.970703125
sparsity Attention v_chunks: 0.9703776041666666
[Time] - Reshape: 258054.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970947265625
sparsity Attention postReshape k_chunks: 0.970703125
sparsity Attention postReshape v_chunks: 0.9703776041666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 288717.0000 nanoseconds
sparsity Attention out: 0.979736328125
[Time] - Reshape Time-Space: 32399.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.9791666666666666
[Time] - Transpose and LIF: 381649.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.683837890625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9693603515625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.67626953125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 407518.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 122736.0000 nanoseconds
sparsity Attention out: 0.9759928385416666
[Time] - Reshape Time-Space: 24195.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 371795.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6739908854166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9694417317708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6642252604166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9658203125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.9658203125
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 256867.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.9658203125
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110482.0000 nanoseconds
sparsity Attention out: 0.972412109375
[Time] - Reshape Time-Space: 22455.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 796214.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0044 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.661865234375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96759033203125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6578776041666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9708658854166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966796875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9708658854166666
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.966796875
[Time] - Reshape: 278703.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9708658854166666
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.966796875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264571.0000 nanoseconds
sparsity Attention out: 0.9746907552083334
[Time] - Reshape Time-Space: 32450.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9742838541666666
sparsity Attention postReshape chunk - 1 output: 0.9847005208333334
sparsity Attention postReshape chunk - 2 output: 0.9690755208333334
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 376479.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.654541015625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665934244791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.656005859375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9650065104166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9650065104166666
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 254544.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9650065104166666
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 111540.0000 nanoseconds
sparsity Attention out: 0.9767252604166666
[Time] - Reshape Time-Space: 25428.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9742838541666666
sparsity Attention postReshape chunk - 1 output: 0.9778645833333334
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9837239583333334
[Time] - Transpose and LIF: 364170.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6603190104166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672037760416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6519368489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96630859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96630859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96630859375
sparsity Attention k_chunks: 0.96630859375
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 257778.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96630859375
sparsity Attention postReshape k_chunks: 0.96630859375
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 143017.0000 nanoseconds
sparsity Attention out: 0.9720865885416666
[Time] - Reshape Time-Space: 22398.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9723307291666666
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9599609375
[Time] - Transpose and LIF: 372027.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9951171875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0048 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.649169921875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673665364583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0024 seconds
sparsity x_for_qkv: 0.6454264322916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 261936.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 261817.0000 nanoseconds
sparsity Attention out: 0.9742838541666666
[Time] - Reshape Time-Space: 30542.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.984375
sparsity Attention postReshape chunk - 1 output: 0.9690755208333334
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 385254.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64599609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9666951497395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6436360677083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9659016927083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966064453125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9639485677083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9659016927083334
sparsity Attention k_chunks: 0.966064453125
sparsity Attention v_chunks: 0.9639485677083334
[Time] - Reshape: 258435.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9659016927083334
sparsity Attention postReshape k_chunks: 0.966064453125
sparsity Attention postReshape v_chunks: 0.9639485677083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253960.0000 nanoseconds
sparsity Attention out: 0.974853515625
[Time] - Reshape Time-Space: 30942.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9720052083333334
sparsity Attention postReshape chunk - 1 output: 0.970703125
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9807942708333334
[Time] - Transpose and LIF: 374076.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9957682291666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6433919270833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676310221354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0539 seconds
Run 20 - Tempo totale del modello: 0.0539 secondi
[Time] - SPS.PSM: 0.0019 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.678466796875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9715983072916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970458984375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.971435546875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9715983072916666
sparsity Attention k_chunks: 0.970458984375
sparsity Attention v_chunks: 0.971435546875
[Time] - Reshape: 413392.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9715983072916666
sparsity Attention postReshape k_chunks: 0.970458984375
sparsity Attention postReshape v_chunks: 0.971435546875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 113500.0000 nanoseconds
sparsity Attention out: 0.9825846354166666
[Time] - Reshape Time-Space: 22446.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9889322916666666
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9820963541666666
sparsity Attention postReshape chunk - 3 output: 0.9791666666666666
[Time] - Transpose and LIF: 363706.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6796061197916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9718831380208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.66796875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968505859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9723307291666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968505859375
sparsity Attention k_chunks: 0.9723307291666666
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 280899.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968505859375
sparsity Attention postReshape k_chunks: 0.9723307291666666
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 261487.0000 nanoseconds
sparsity Attention out: 0.980712890625
[Time] - Reshape Time-Space: 30869.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.978515625
[Time] - Transpose and LIF: 357421.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6624348958333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9698079427083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6566569010416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9696451822916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970703125
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.9696451822916666
[Time] - Reshape: 251571.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970703125
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.9696451822916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260136.0000 nanoseconds
sparsity Attention out: 0.9785970052083334
[Time] - Reshape Time-Space: 29755.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9798177083333334
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 368805.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6504720052083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686279296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6498209635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970458984375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9693196614583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.970458984375
sparsity Attention v_chunks: 0.9693196614583334
[Time] - Reshape: 264105.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.970458984375
sparsity Attention postReshape v_chunks: 0.9693196614583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 111497.0000 nanoseconds
sparsity Attention out: 0.9800618489583334
[Time] - Reshape Time-Space: 23561.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9860026041666666
[Time] - Transpose and LIF: 371533.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6451822916666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686686197916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6433919270833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96875
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 282277.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96875
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 266178.0000 nanoseconds
sparsity Attention out: 0.9795735677083334
[Time] - Reshape Time-Space: 30439.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9827473958333334
[Time] - Transpose and LIF: 360452.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0044 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6436360677083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686482747395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6436360677083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 251589.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258937.0000 nanoseconds
sparsity Attention out: 0.9794921875
[Time] - Reshape Time-Space: 31329.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 372580.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6437174479166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670003255208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6432291666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 312149.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110876.0000 nanoseconds
sparsity Attention out: 0.974609375
[Time] - Reshape Time-Space: 22445.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.978515625
sparsity Attention postReshape chunk - 1 output: 0.9690755208333334
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.9830729166666666
[Time] - Transpose and LIF: 372062.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6412760416666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670206705729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6416015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9691569010416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9654134114583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.9691569010416666
sparsity Attention v_chunks: 0.9654134114583334
[Time] - Reshape: 264342.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.9691569010416666
sparsity Attention postReshape v_chunks: 0.9654134114583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 112006.0000 nanoseconds
sparsity Attention out: 0.9749348958333334
[Time] - Reshape Time-Space: 22797.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.96484375
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9860026041666666
[Time] - Transpose and LIF: 353859.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6427408854166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9668375651041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0532 seconds
Run 21 - Tempo totale del modello: 0.0532 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6853841145833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9693196614583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.9693196614583334
[Time] - Reshape: 333324.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.9693196614583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 266271.0000 nanoseconds
sparsity Attention out: 0.979736328125
[Time] - Reshape Time-Space: 29814.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9892578125
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.9664713541666666
sparsity Attention postReshape chunk - 3 output: 0.9794921875
[Time] - Transpose and LIF: 378055.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.675537109375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9707845052083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6680501302083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970947265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970703125
sparsity Attention k_chunks: 0.970947265625
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 252364.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970703125
sparsity Attention postReshape k_chunks: 0.970947265625
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 249911.0000 nanoseconds
sparsity Attention out: 0.9794921875
[Time] - Reshape Time-Space: 29836.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 383329.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.670654296875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9693400065104166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6664225260416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9686686197916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96533203125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9686686197916666
sparsity Attention k_chunks: 0.96533203125
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 266424.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9686686197916666
sparsity Attention postReshape k_chunks: 0.96533203125
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109818.0000 nanoseconds
sparsity Attention out: 0.9772135416666666
[Time] - Reshape Time-Space: 22288.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9817708333333334
sparsity Attention postReshape chunk - 3 output: 0.9664713541666666
[Time] - Transpose and LIF: 376261.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.665283203125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96923828125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.662109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698893229166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9671223958333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698893229166666
sparsity Attention k_chunks: 0.9671223958333334
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 282183.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698893229166666
sparsity Attention postReshape k_chunks: 0.9671223958333334
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 267109.0000 nanoseconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 32603.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9765625
sparsity Attention postReshape chunk - 1 output: 0.97265625
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9827473958333334
[Time] - Transpose and LIF: 368720.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6595865885416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680989583333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.656005859375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9679361979166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9680989583333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.9679361979166666
sparsity Attention v_chunks: 0.9680989583333334
[Time] - Reshape: 258026.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.9679361979166666
sparsity Attention postReshape v_chunks: 0.9680989583333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264097.0000 nanoseconds
sparsity Attention out: 0.9771321614583334
[Time] - Reshape Time-Space: 30895.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 378524.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.655517578125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9666951497395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6484375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96630859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96630859375
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 294215.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96630859375
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 112408.0000 nanoseconds
sparsity Attention out: 0.9752604166666666
[Time] - Reshape Time-Space: 23135.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9674479166666666
[Time] - Transpose and LIF: 368416.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64501953125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670206705729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6488444010416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9679361979166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96630859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9679361979166666
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.96630859375
[Time] - Reshape: 258330.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9679361979166666
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.96630859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110857.0000 nanoseconds
sparsity Attention out: 0.9716796875
[Time] - Reshape Time-Space: 23366.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9573567708333334
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 353341.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.642822265625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677530924479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6412760416666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9662272135416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9700520833333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.9662272135416666
sparsity Attention v_chunks: 0.9700520833333334
[Time] - Reshape: 253530.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.9662272135416666
sparsity Attention postReshape v_chunks: 0.9700520833333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 267549.0000 nanoseconds
sparsity Attention out: 0.974853515625
[Time] - Reshape Time-Space: 30045.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9729817708333334
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.98046875
[Time] - Transpose and LIF: 370430.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.994140625
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.638916015625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96734619140625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0530 seconds
Run 22 - Tempo totale del modello: 0.0530 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6778971354166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970947265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970458984375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.970947265625
sparsity Attention v_chunks: 0.970458984375
[Time] - Reshape: 262900.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.970947265625
sparsity Attention postReshape v_chunks: 0.970458984375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259249.0000 nanoseconds
sparsity Attention out: 0.97607421875
[Time] - Reshape Time-Space: 30492.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98828125
sparsity Attention postReshape chunk - 1 output: 0.970703125
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 387696.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.67333984375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.971435546875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6665852864583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 289282.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 254109.0000 nanoseconds
sparsity Attention out: 0.9742838541666666
[Time] - Reshape Time-Space: 29528.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9765625
sparsity Attention postReshape chunk - 3 output: 0.962890625
[Time] - Transpose and LIF: 467100.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6647135416666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9700724283854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6570638020833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970947265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.970947265625
[Time] - Reshape: 256875.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.970947265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 118471.0000 nanoseconds
sparsity Attention out: 0.97705078125
[Time] - Reshape Time-Space: 24217.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9716796875
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.982421875
sparsity Attention postReshape chunk - 3 output: 0.9742838541666666
[Time] - Transpose and LIF: 370512.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6555989583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9721883138020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.65185546875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966552734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966552734375
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 258041.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.966552734375
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260180.0000 nanoseconds
sparsity Attention out: 0.9742838541666666
[Time] - Reshape Time-Space: 30054.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9703776041666666
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 407806.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6516927083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681193033854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6513671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9663899739583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9683430989583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9663899739583334
sparsity Attention v_chunks: 0.9683430989583334
[Time] - Reshape: 282292.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9663899739583334
sparsity Attention postReshape v_chunks: 0.9683430989583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 251502.0000 nanoseconds
sparsity Attention out: 0.9727376302083334
[Time] - Reshape Time-Space: 27113.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9729817708333334
sparsity Attention postReshape chunk - 1 output: 0.9710286458333334
sparsity Attention postReshape chunk - 2 output: 0.9794921875
sparsity Attention postReshape chunk - 3 output: 0.9674479166666666
[Time] - Transpose and LIF: 362163.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9964192708333334
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6520182291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679361979166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.644287109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 398456.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 111369.0000 nanoseconds
sparsity Attention out: 0.9781087239583334
[Time] - Reshape Time-Space: 22718.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9853515625
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 370637.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6464029947916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677937825520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6412760416666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9664713541666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9659830729166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.9664713541666666
sparsity Attention v_chunks: 0.9659830729166666
[Time] - Reshape: 266111.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.9664713541666666
sparsity Attention postReshape v_chunks: 0.9659830729166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260146.0000 nanoseconds
sparsity Attention out: 0.9745279947916666
[Time] - Reshape Time-Space: 30383.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9632161458333334
sparsity Attention postReshape chunk - 1 output: 0.9723307291666666
sparsity Attention postReshape chunk - 2 output: 0.9856770833333334
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 379785.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6409505208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679158528645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6402994791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9667154947916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.9667154947916666
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 258040.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.9667154947916666
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 250217.0000 nanoseconds
sparsity Attention out: 0.9737955729166666
[Time] - Reshape Time-Space: 30291.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 377182.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6431477864583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672648111979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
[Time] - Spikeformer Time time: 0.0525 seconds
Run 23 - Tempo totale del modello: 0.0525 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.671630859375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9715169270833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.971435546875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.9715169270833334
sparsity Attention v_chunks: 0.971435546875
[Time] - Reshape: 320518.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.9715169270833334
sparsity Attention postReshape v_chunks: 0.971435546875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 113067.0000 nanoseconds
sparsity Attention out: 0.9813639322916666
[Time] - Reshape Time-Space: 22796.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9788411458333334
[Time] - Transpose and LIF: 372533.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9944661458333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6765950520833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9712320963541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6656087239583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9702962239583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9703776041666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.9702962239583334
sparsity Attention v_chunks: 0.9703776041666666
[Time] - Reshape: 252867.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.9702962239583334
sparsity Attention postReshape v_chunks: 0.9703776041666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108627.0000 nanoseconds
sparsity Attention out: 0.979736328125
[Time] - Reshape Time-Space: 22449.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9873046875
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.9807942708333334
[Time] - Transpose and LIF: 364009.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6722005208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9704182942708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6673177083333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9678548177083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9678548177083334
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 262312.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9678548177083334
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255289.0000 nanoseconds
sparsity Attention out: 0.9733072916666666
[Time] - Reshape Time-Space: 29841.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.962890625
sparsity Attention postReshape chunk - 3 output: 0.9778645833333334
[Time] - Transpose and LIF: 371862.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6644694010416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679768880208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6578776041666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9691569010416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970947265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.9691569010416666
sparsity Attention v_chunks: 0.970947265625
[Time] - Reshape: 254259.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.9691569010416666
sparsity Attention postReshape v_chunks: 0.970947265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255649.0000 nanoseconds
sparsity Attention out: 0.977783203125
[Time] - Reshape Time-Space: 31532.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 381924.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6521809895833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677734375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.65185546875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967529296875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698893229166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967529296875
sparsity Attention k_chunks: 0.9698893229166666
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 316207.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967529296875
sparsity Attention postReshape k_chunks: 0.9698893229166666
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110297.0000 nanoseconds
sparsity Attention out: 0.975830078125
[Time] - Reshape Time-Space: 23205.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 373932.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6494954427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673055013020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.64453125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9703776041666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698893229166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9703776041666666
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.9698893229166666
[Time] - Reshape: 260744.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9703776041666666
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.9698893229166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 257606.0000 nanoseconds
sparsity Attention out: 0.9755859375
[Time] - Reshape Time-Space: 29592.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 377231.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0046 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.640625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9692179361979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6356608072916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9686686197916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9662272135416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9686686197916666
sparsity Attention k_chunks: 0.9662272135416666
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 254734.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9686686197916666
sparsity Attention postReshape k_chunks: 0.9662272135416666
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 252564.0000 nanoseconds
sparsity Attention out: 0.9762369791666666
[Time] - Reshape Time-Space: 29517.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.9713541666666666
sparsity Attention postReshape chunk - 2 output: 0.9680989583333334
sparsity Attention postReshape chunk - 3 output: 0.9820963541666666
[Time] - Transpose and LIF: 355327.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6377766927083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677937825520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6344401041666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966064453125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9693196614583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966064453125
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9693196614583334
[Time] - Reshape: 258134.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.966064453125
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9693196614583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109573.0000 nanoseconds
sparsity Attention out: 0.9781087239583334
[Time] - Reshape Time-Space: 24726.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9733072916666666
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9811197916666666
[Time] - Transpose and LIF: 375152.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.634033203125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683837890625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0532 seconds
Run 24 - Tempo totale del modello: 0.0532 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6787923177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9702962239583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9722493489583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96923828125
sparsity Attention k_chunks: 0.9702962239583334
sparsity Attention v_chunks: 0.9722493489583334
[Time] - Reshape: 295982.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96923828125
sparsity Attention postReshape k_chunks: 0.9702962239583334
sparsity Attention postReshape v_chunks: 0.9722493489583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110191.0000 nanoseconds
sparsity Attention out: 0.9798990885416666
[Time] - Reshape Time-Space: 24647.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9873046875
sparsity Attention postReshape chunk - 2 output: 0.9781901041666666
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 377765.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6704915364583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690348307291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6621907552083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970947265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970947265625
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 255348.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970947265625
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 313167.0000 nanoseconds
sparsity Attention out: 0.978515625
[Time] - Reshape Time-Space: 30225.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9703776041666666
sparsity Attention postReshape chunk - 3 output: 0.9833984375
[Time] - Transpose and LIF: 380999.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6577962239583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690755208333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.651611328125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9679361979166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9696451822916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9679361979166666
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.9696451822916666
[Time] - Reshape: 254135.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9679361979166666
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.9696451822916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 111808.0000 nanoseconds
sparsity Attention out: 0.9775390625
[Time] - Reshape Time-Space: 23605.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 354495.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6527506510416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9696858723958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6481119791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698893229166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9671223958333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698893229166666
sparsity Attention k_chunks: 0.9671223958333334
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 313260.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698893229166666
sparsity Attention postReshape k_chunks: 0.9671223958333334
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 111460.0000 nanoseconds
sparsity Attention out: 0.9729817708333334
[Time] - Reshape Time-Space: 22625.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.9671223958333334
sparsity Attention postReshape chunk - 3 output: 0.9677734375
[Time] - Transpose and LIF: 357840.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6503092447916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681803385416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6460774739583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 255260.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 262530.0000 nanoseconds
sparsity Attention out: 0.97900390625
[Time] - Reshape Time-Space: 31266.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.984375
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 376772.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6426595052083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673258463541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6446940104166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9677734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9677734375
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 312534.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9677734375
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 257975.0000 nanoseconds
sparsity Attention out: 0.9722493489583334
[Time] - Reshape Time-Space: 29827.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9736328125
sparsity Attention postReshape chunk - 1 output: 0.9710286458333334
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 376346.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6429036458333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677530924479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.63916015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 262827.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108963.0000 nanoseconds
sparsity Attention out: 0.9745279947916666
[Time] - Reshape Time-Space: 22950.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9755859375
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 368025.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.636962890625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677327473958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6358235677083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9662272135416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.965087890625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9662272135416666
sparsity Attention k_chunks: 0.965087890625
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 284199.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9662272135416666
sparsity Attention postReshape k_chunks: 0.965087890625
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 262700.0000 nanoseconds
sparsity Attention out: 0.9739583333333334
[Time] - Reshape Time-Space: 30025.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9739583333333334
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 366433.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.637451171875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685262044270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0535 seconds
Run 25 - Tempo totale del modello: 0.0535 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6822102864583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9705403645833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9707845052083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9705403645833334
sparsity Attention k_chunks: 0.97265625
sparsity Attention v_chunks: 0.9707845052083334
[Time] - Reshape: 260079.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9705403645833334
sparsity Attention postReshape k_chunks: 0.97265625
sparsity Attention postReshape v_chunks: 0.9707845052083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 257546.0000 nanoseconds
sparsity Attention out: 0.9838053385416666
[Time] - Reshape Time-Space: 29470.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9931640625
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 395847.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6790364583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9715169270833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6676432291666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.971435546875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.971435546875
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 260351.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.971435546875
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255792.0000 nanoseconds
sparsity Attention out: 0.9800618489583334
[Time] - Reshape Time-Space: 32262.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9892578125
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9752604166666666
[Time] - Transpose and LIF: 374411.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6661783854166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9699300130208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6609700520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 364156.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 114822.0000 nanoseconds
sparsity Attention out: 0.97509765625
[Time] - Reshape Time-Space: 22590.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9615885416666666
[Time] - Transpose and LIF: 354351.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96929931640625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.653076171875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9710286458333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9700520833333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9710286458333334
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.9700520833333334
[Time] - Reshape: 259105.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9710286458333334
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.9700520833333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258476.0000 nanoseconds
sparsity Attention out: 0.9838053385416666
[Time] - Reshape Time-Space: 30305.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.986328125
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.9833984375
sparsity Attention postReshape chunk - 3 output: 0.9772135416666666
[Time] - Transpose and LIF: 382063.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6556803385416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9692789713541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6512858072916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9657389322916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9664713541666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9657389322916666
sparsity Attention v_chunks: 0.9664713541666666
[Time] - Reshape: 263275.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9657389322916666
sparsity Attention postReshape v_chunks: 0.9664713541666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 267541.0000 nanoseconds
sparsity Attention out: 0.9710286458333334
[Time] - Reshape Time-Space: 29139.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9703776041666666
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9635416666666666
[Time] - Transpose and LIF: 364482.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9954427083333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6480305989583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677734375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6486002604166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 302971.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110575.0000 nanoseconds
sparsity Attention out: 0.9765625
[Time] - Reshape Time-Space: 22908.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 355599.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6471354166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96856689453125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6460774739583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9688313802083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.9688313802083334
[Time] - Reshape: 256739.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.9688313802083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259559.0000 nanoseconds
sparsity Attention out: 0.9763997395833334
[Time] - Reshape Time-Space: 29134.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9772135416666666
sparsity Attention postReshape chunk - 3 output: 0.9752604166666666
[Time] - Transpose and LIF: 386384.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6444498697916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683634440104166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6444498697916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96728515625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.96728515625
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 260225.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.96728515625
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 251850.0000 nanoseconds
sparsity Attention out: 0.97509765625
[Time] - Reshape Time-Space: 29716.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9684244791666666
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.9850260416666666
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 379000.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6444498697916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9658610026041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0532 seconds
Run 26 - Tempo totale del modello: 0.0532 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.67578125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9697265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97216796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97021484375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9697265625
sparsity Attention k_chunks: 0.97216796875
sparsity Attention v_chunks: 0.97021484375
[Time] - Reshape: 254637.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9697265625
sparsity Attention postReshape k_chunks: 0.97216796875
sparsity Attention postReshape v_chunks: 0.97021484375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 262213.0000 nanoseconds
sparsity Attention out: 0.982421875
[Time] - Reshape Time-Space: 30810.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.9781901041666666
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 382297.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6680501302083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96978759765625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6634114583333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9700520833333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970458984375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9700520833333334
sparsity Attention v_chunks: 0.970458984375
[Time] - Reshape: 316890.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9700520833333334
sparsity Attention postReshape v_chunks: 0.970458984375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110895.0000 nanoseconds
sparsity Attention out: 0.9805501302083334
[Time] - Reshape Time-Space: 22992.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9847005208333334
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9830729166666666
[Time] - Transpose and LIF: 364796.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.664794921875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96942138671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.662109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9701334635416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.9701334635416666
sparsity Attention v_chunks: 0.970703125
[Time] - Reshape: 255488.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.9701334635416666
sparsity Attention postReshape v_chunks: 0.970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258749.0000 nanoseconds
sparsity Attention out: 0.9749348958333334
[Time] - Reshape Time-Space: 28036.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9697265625
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 360919.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6565755208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677530924479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6569010416666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96630859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96630859375
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 256617.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96630859375
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 249889.0000 nanoseconds
sparsity Attention out: 0.9729817708333334
[Time] - Reshape Time-Space: 30118.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9720052083333334
sparsity Attention postReshape chunk - 1 output: 0.9703776041666666
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 358527.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6443684895833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676106770833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6448567708333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 251947.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 116001.0000 nanoseconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 22307.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9765625
sparsity Attention postReshape chunk - 1 output: 0.9778645833333334
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9778645833333334
[Time] - Transpose and LIF: 445768.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.645751953125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686075846354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6421712239583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9672037760416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.9672037760416666
[Time] - Reshape: 257485.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.9672037760416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258912.0000 nanoseconds
sparsity Attention out: 0.9771321614583334
[Time] - Reshape Time-Space: 29949.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 383620.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.641845703125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9664713541666666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6416015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966552734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9663899739583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.966552734375
sparsity Attention v_chunks: 0.9663899739583334
[Time] - Reshape: 293023.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.966552734375
sparsity Attention postReshape v_chunks: 0.9663899739583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 247704.0000 nanoseconds
sparsity Attention out: 0.975830078125
[Time] - Reshape Time-Space: 29685.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9869791666666666
sparsity Attention postReshape chunk - 1 output: 0.9680989583333334
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 379226.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6422526041666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678751627604166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.63623046875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 254510.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 116977.0000 nanoseconds
sparsity Attention out: 0.9785970052083334
[Time] - Reshape Time-Space: 23853.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9798177083333334
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 371980.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9947916666666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6375325520833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9667154947916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
[Time] - Spikeformer Time time: 0.0529 seconds
Run 27 - Tempo totale del modello: 0.0530 secondi
[Time] - SPS.PSM: 0.0019 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.676025390625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9707845052083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9717610677083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9707845052083334
sparsity Attention v_chunks: 0.9717610677083334
[Time] - Reshape: 293565.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9707845052083334
sparsity Attention postReshape v_chunks: 0.9717610677083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 111682.0000 nanoseconds
sparsity Attention out: 0.9838053385416666
[Time] - Reshape Time-Space: 24396.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98828125
sparsity Attention postReshape chunk - 1 output: 0.986328125
sparsity Attention postReshape chunk - 2 output: 0.9850260416666666
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 369331.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.67626953125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9708658854166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.66455078125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9695638020833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9695638020833334
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 261135.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9695638020833334
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264537.0000 nanoseconds
sparsity Attention out: 0.9803059895833334
[Time] - Reshape Time-Space: 30712.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.984375
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9837239583333334
[Time] - Transpose and LIF: 363508.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6668294270833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9691975911458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6629231770833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9679361979166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.9679361979166666
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 255254.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.9679361979166666
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253888.0000 nanoseconds
sparsity Attention out: 0.975341796875
[Time] - Reshape Time-Space: 32203.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.97265625
sparsity Attention postReshape chunk - 1 output: 0.9791666666666666
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 382280.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6591796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688924153645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.654052734375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9662272135416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96630859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9662272135416666
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.96630859375
[Time] - Reshape: 256547.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9662272135416666
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.96630859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109693.0000 nanoseconds
sparsity Attention out: 0.9727376302083334
[Time] - Reshape Time-Space: 22948.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9609375
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 371145.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6538899739583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688313802083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6502278645833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 255781.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264272.0000 nanoseconds
sparsity Attention out: 0.9794108072916666
[Time] - Reshape Time-Space: 30421.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.9895833333333334
sparsity Attention postReshape chunk - 2 output: 0.9651692708333334
sparsity Attention postReshape chunk - 3 output: 0.98046875
[Time] - Transpose and LIF: 379869.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6471354166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677327473958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6472981770833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96630859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9659830729166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96630859375
sparsity Attention k_chunks: 0.9659830729166666
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 309788.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96630859375
sparsity Attention postReshape k_chunks: 0.9659830729166666
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255989.0000 nanoseconds
sparsity Attention out: 0.9742024739583334
[Time] - Reshape Time-Space: 29402.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 370368.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6466471354166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686075846354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6451822916666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9662272135416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.9662272135416666
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 258332.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.9662272135416666
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 115049.0000 nanoseconds
sparsity Attention out: 0.9720052083333334
[Time] - Reshape Time-Space: 24031.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.970703125
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 374945.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6397298177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.966552734375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.63818359375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9657389322916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96533203125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9657389322916666
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.96533203125
[Time] - Reshape: 285647.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9657389322916666
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.96533203125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108625.0000 nanoseconds
sparsity Attention out: 0.9663899739583334
[Time] - Reshape Time-Space: 28875.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9635416666666666
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9654947916666666
sparsity Attention postReshape chunk - 3 output: 0.958984375
[Time] - Transpose and LIF: 372501.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9912109375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6351725260416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9663289388020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
[Time] - Spikeformer Time time: 0.0532 seconds
Run 28 - Tempo totale del modello: 0.0532 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0009 seconds
sparsity x_for_qkv: 0.6790364583333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9734700520833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9717610677083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9701334635416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9734700520833334
sparsity Attention k_chunks: 0.9717610677083334
sparsity Attention v_chunks: 0.9701334635416666
[Time] - Reshape: 369450.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9734700520833334
sparsity Attention postReshape k_chunks: 0.9717610677083334
sparsity Attention postReshape v_chunks: 0.9701334635416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 286448.0000 nanoseconds
sparsity Attention out: 0.9806315104166666
[Time] - Reshape Time-Space: 31646.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9899088541666666
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 375143.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9947916666666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6663411458333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9707845052083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.661376953125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9718424479166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.971435546875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9720052083333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9718424479166666
sparsity Attention k_chunks: 0.971435546875
sparsity Attention v_chunks: 0.9720052083333334
[Time] - Reshape: 256558.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9718424479166666
sparsity Attention postReshape k_chunks: 0.971435546875
sparsity Attention postReshape v_chunks: 0.9720052083333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 252007.0000 nanoseconds
sparsity Attention out: 0.9806315104166666
[Time] - Reshape Time-Space: 30022.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9791666666666666
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 396887.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6607259114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690144856770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.65576171875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9696451822916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.9696451822916666
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 267284.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.9696451822916666
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110252.0000 nanoseconds
sparsity Attention out: 0.9774576822916666
[Time] - Reshape Time-Space: 22231.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9729817708333334
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9830729166666666
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 364360.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9954427083333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.652587890625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690755208333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6513671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9712727864583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9712727864583334
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 263874.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9712727864583334
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 392389.0000 nanoseconds
sparsity Attention out: 0.97900390625
[Time] - Reshape Time-Space: 31362.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9755859375
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9801432291666666
[Time] - Transpose and LIF: 375992.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.1343 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64697265625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680582682291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6451822916666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968017578125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9656575520833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96923828125
sparsity Attention k_chunks: 0.968017578125
sparsity Attention v_chunks: 0.9656575520833334
[Time] - Reshape: 282944.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96923828125
sparsity Attention postReshape k_chunks: 0.968017578125
sparsity Attention postReshape v_chunks: 0.9656575520833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 1318937.0000 nanoseconds
sparsity Attention out: 0.9749348958333334
[Time] - Reshape Time-Space: 41993.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9677734375
sparsity Attention postReshape chunk - 1 output: 0.9703776041666666
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.9830729166666666
[Time] - Transpose and LIF: 402547.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0051 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6466471354166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96826171875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6444498697916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9688313802083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9688313802083334
[Time] - Reshape: 262473.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9688313802083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 111893.0000 nanoseconds
sparsity Attention out: 0.97900390625
[Time] - Reshape Time-Space: 21024.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.978515625
sparsity Attention postReshape chunk - 1 output: 0.9742838541666666
sparsity Attention postReshape chunk - 2 output: 0.986328125
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 372190.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6402994791666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677327473958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.639892578125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.965576171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9673665364583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.965576171875
sparsity Attention k_chunks: 0.9673665364583334
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 260340.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.965576171875
sparsity Attention postReshape k_chunks: 0.9673665364583334
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107705.0000 nanoseconds
sparsity Attention out: 0.9746907552083334
[Time] - Reshape Time-Space: 21849.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9700520833333334
sparsity Attention postReshape chunk - 1 output: 0.9876302083333334
sparsity Attention postReshape chunk - 2 output: 0.9658203125
sparsity Attention postReshape chunk - 3 output: 0.9752604166666666
[Time] - Transpose and LIF: 370851.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6399739583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688517252604166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0026 seconds
sparsity x_for_qkv: 0.6346842447916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9664713541666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.9664713541666666
[Time] - Reshape: 271326.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.9664713541666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 266007.0000 nanoseconds
sparsity Attention out: 0.9728190104166666
[Time] - Reshape Time-Space: 29948.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9664713541666666
sparsity Attention postReshape chunk - 1 output: 0.9729817708333334
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9768880208333334
[Time] - Transpose and LIF: 393888.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6336263020833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.967041015625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
[Time] - Spikeformer Time time: 0.1853 seconds
Run 29 - Tempo totale del modello: 0.1853 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6817220052083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9708658854166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9706217447916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9708658854166666
sparsity Attention v_chunks: 0.9706217447916666
[Time] - Reshape: 303792.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9708658854166666
sparsity Attention postReshape v_chunks: 0.9706217447916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 257295.0000 nanoseconds
sparsity Attention out: 0.9781901041666666
[Time] - Reshape Time-Space: 29520.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9765625
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 383883.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.683349609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9704996744791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6717122395833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9708658854166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9708658854166666
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 259401.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9708658854166666
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 112172.0000 nanoseconds
sparsity Attention out: 0.9798990885416666
[Time] - Reshape Time-Space: 23135.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9817708333333334
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 380881.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66943359375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9692179361979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6669921875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 256282.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264498.0000 nanoseconds
sparsity Attention out: 0.9777018229166666
[Time] - Reshape Time-Space: 30892.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9654947916666666
[Time] - Transpose and LIF: 369381.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.99609375
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65869140625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675089518229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6612141927083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9664713541666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9664713541666666
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 266622.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9664713541666666
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 289972.0000 nanoseconds
sparsity Attention out: 0.97412109375
[Time] - Reshape Time-Space: 32631.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9658203125
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 385047.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6590983072916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671223958333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6548665364583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967529296875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967529296875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967529296875
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.967529296875
[Time] - Reshape: 290543.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967529296875
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.967529296875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 118650.0000 nanoseconds
sparsity Attention out: 0.9757486979166666
[Time] - Reshape Time-Space: 23478.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.9716796875
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 380550.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6509602864583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9664713541666666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6534016927083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9703776041666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9659830729166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9703776041666666
sparsity Attention k_chunks: 0.9659830729166666
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 379117.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9703776041666666
sparsity Attention postReshape k_chunks: 0.9659830729166666
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 110067.0000 nanoseconds
sparsity Attention out: 0.9742024739583334
[Time] - Reshape Time-Space: 22735.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9847005208333334
sparsity Attention postReshape chunk - 2 output: 0.9606119791666666
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 381580.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6475423177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676310221354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6435546875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9666341145833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967529296875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9666341145833334
sparsity Attention v_chunks: 0.967529296875
[Time] - Reshape: 320752.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9666341145833334
sparsity Attention postReshape v_chunks: 0.967529296875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 262915.0000 nanoseconds
sparsity Attention out: 0.9785970052083334
[Time] - Reshape Time-Space: 30902.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9749348958333334
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9765625
sparsity Attention postReshape chunk - 3 output: 0.9817708333333334
[Time] - Transpose and LIF: 465093.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6380208333333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9668375651041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6387532552083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96435546875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.96435546875
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 264155.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.96435546875
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258485.0000 nanoseconds
sparsity Attention out: 0.982177734375
[Time] - Reshape Time-Space: 34051.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.9817708333333334
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 391042.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6402180989583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672444661458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
[Time] - Spikeformer Time time: 0.0541 seconds
Run 30 - Tempo totale del modello: 0.0541 secondi

# Fork of the STAtten project
This modified version of the STAtten model includes timers and sparsity measuring solutions inside the model with the aim of profilation.
When executed through the model.ipynb the output generated is aggregated in .txt files that are then read and elaborated with other scripts that can be found here:

<link>
https://github.com/MarcoCrisafulli5/STAtten_profiler
</link>


## Modified Files:
```
/model/spikeformer.py # now includes timers in the forward function call
/module/sps.py # now includes times to measure the execution time of the sps block and its sub blocks
/module/ms_conv.py # now includes timers to measure the Encoder execution time and sparsity is measured in different points inside the encoder block
```

## The Output
The output .txt files that were mentioned before are present in this repo for representation purpose and are not the ones used to analyze the data. The generic structure of the files is similar to those used in my other <link src="https://github.com/MarcoCrisafulli5/STAtten_profiler">project</link>.
```
capcifar.txt   # generic capture using CIFAR dataset
capcifar3.txt   # generic capture using CIFAR dataset
capcifarTensRandn.txt   # generic capture using RANDOM tensor generated 
```




# STAtten: Spiking Transformer with Spatial-Temporal Attention [[Paper]](https://arxiv.org/pdf/2409.19764)
### ***Conference on Computer Vision and Pattern Recognition (CVPR), 2025***

![method.png](images/method.png)

[//]: # ([![License]&#40;https://img.shields.io/badge/License-MIT-yellow.svg&#41;]&#40;LICENSE&#41;)



[//]: # (## Table of Contents)

[//]: # (- [Overview]&#40;#overview&#41;)

[//]: # (- [Paper]&#40;#paper&#41;)

[//]: # (- [Installation]&#40;#installation&#41;)

[//]: # (- [Usage]&#40;#usage&#41;)

[//]: # (- [Training Details]&#40;#training-details&#41;)

[//]: # (- [Dependencies]&#40;#dependencies&#41;)

[//]: # (- [Contributing]&#40;#contributing&#41;)

[//]: # (- [Citation]&#40;#citation&#41;)

## Abstract
Spike-based Transformer presents a compelling and energy-efficient alternative to traditional Artificial Neural Network (ANN)-based Transformers, achieving impressive results through sparse binary computations. However, existing spike-based transformers predominantly focus on spatial attention while neglecting crucial temporal dependencies inherent in spike-based processing, leading to suboptimal feature representation and limited performance. To address this limitation, we propose Spiking Transformer with **S**patial-**T**emporal **Atten**tion (**STAtten**), a simple and straightforward architecture that efficiently integrates both spatial and temporal information in the self-attention mechanism. STAtten introduces a block-wise computation strategy that processes information in spatial-temporal chunks, enabling comprehensive feature capture while maintaining the same computational complexity as previous spatial-only approaches. Our method can be seamlessly integrated into existing spike-based transformers without architectural overhaul. Extensive experiments demonstrate that STAtten significantly improves the performance of existing spike-based transformers across both static and neuromorphic datasets, including CIFAR10/100, ImageNet, CIFAR10-DVS, and N-Caltech101. 

## Dependency
- timm == 0.6.12
- pytorch == 1.13.1
- cupy
- spikingjelly == 0.0.0.0.12
- tensorboard

## Data 
- CIFAR10-DVS: Download through Spikingjelly framework
- N-Caltech101: Download through Spikingjelly framework
- ImageNet: https://www.image-net.org/index.php

   ```bash
   conf
   â”œâ”€â”€ ...
   data
   â”œâ”€â”€ CIFAR10/100
   â”‚   â””â”€â”€ ...
   â”œâ”€â”€ CIFAR10-DVS
   â”‚   â”œâ”€â”€ frames_number_10_split_by_number
   â”‚   â””â”€â”€ frames_number_16_split_by_number
   â”œâ”€â”€ N-Caltech101
   â”‚   â”œâ”€â”€ frames_number_10_split_by_number
   â”‚   â””â”€â”€ frames_number_16_split_by_number
   â”œâ”€â”€ ImageNet
   â”‚   â””â”€â”€ ...


## Usage
### Train
Replace the "-c" for setting the configurations
- **CIFAR10/100**
   ```bash
   CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 train.py -c conf/cifar10/2_512_200E_t4.yml --model sdt --spike-mode lif --attention_mode STAtten
- **CIFAR10-DVS**
   ```bash
   CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 train.py -c conf/cifar10-dvs/2_256_200E_t16_TET.yml --model sdt --spike-mode lif --attention_mode STAtten
- **N-Caltech101**
   ```bash
   CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 train.py -c conf/ncaltech101/2_256_200E_t16.yml --model sdt --spike-mode lif --attention_mode STAtten
- **ImageNet**
   ```bash
   CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 --master_port 29500 train.py -c conf/imagenet/8_768_200E_t4.yml --model sdt --spike-mode lif --attention_mode STAtten

### Test
Replace the "--resume" with your trained checkpoints
- **ImageNet**
   ```bash
   CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 --master_port 29500 test.py -c conf/imagenet/8_768_200E_t4.yml --model sdt --spike-mode lif --attention_mode STAtten --resume {checkpoints} --no-resume-opt

## Ackonwledgements
Our code is based on below repositories. Thank you for your valuable works!
- Spike-driven Transformer: https://github.com/BICLab/Spike-Driven-Transformer
- Spikingjelly: https://github.com/fangwei123456/spikingjelly

## Citation <span style="color:blue">ðŸ“š</span>
If you find our "Spiking Transformer with Spatial-Temporal Attention" paper useful or relevant to your research, please kindly cite our paper:
  ```bash
  @inproceedings{lee2025spiking,
  title={Spiking transformer with spatial-temporal attention},
  author={Lee, Donghyun and Li, Yuhang and Kim, Youngeun and Xiao, Shiting and Panda, Priyadarshini},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={13948--13958},
  year={2025}
}
